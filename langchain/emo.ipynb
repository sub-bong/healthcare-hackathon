{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100","mount_file_id":"19Y3JCtGJzbmzIaeNafZjxY1IwvDEKps3","authorship_tag":"ABX9TyPGk3xDp+ZQVG//yw2o2Aop"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hHytXytMk4Rn","executionInfo":{"status":"ok","timestamp":1758601139019,"user_tz":-540,"elapsed":11646,"user":{"displayName":"김은지","userId":"00400322563815083189"}},"outputId":"955c4448-e0ad-42a8-e8b5-759a7832ee3b","collapsed":true},"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n","0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.8/38.8 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m95.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["# ffmpeg\n","!apt -y install ffmpeg\n","# 파이썬 패키지\n","!pip -q install faster-whisper python-dotenv openai"]},{"cell_type":"code","source":["%%writefile stt.py\n","import os, tempfile, subprocess\n","from transformers import pipeline\n","import torch\n","\n","# 1) ffmpeg로 비디오→WAV(PCM 16bit)만 뽑기\n","def extract_audio_pcm16(input_video: str) -> str:\n","    td = tempfile.mkdtemp(prefix=\"stt_\")\n","    wav_path = os.path.join(td, \"audio.wav\")\n","    cmd = [\"ffmpeg\", \"-y\", \"-i\", input_video, \"-vn\", \"-c:a\", \"pcm_s16le\", wav_path]\n","    subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n","    return wav_path  # 임시 wav 경로 반환\n","\n","# 2) Whisper 파이프라인(긴 파일도 청크로 자동 처리)\n","ASR_MODEL  = \"openai/whisper-small\"  # 필요시 medium/large\n","ASR_DEVICE = 0 if torch.cuda.is_available() else -1\n","asr = pipeline(\n","    \"automatic-speech-recognition\",\n","    model=ASR_MODEL,\n","    device=ASR_DEVICE,\n","    chunk_length_s=30,\n","    generate_kwargs={\"task\":\"transcribe\", \"language\":\"<|ko|>\"}\n",")\n","\n","# 3) 엔드투엔드: 영상 경로만 넣으면 텍스트 반환\n","def transcribe_video(video_path: str) -> str:\n","    wav_path = extract_audio_pcm16(video_path)\n","    try:\n","        text = asr(wav_path)[\"text\"]  # 전사 결과만 뽑아서\n","        return text                   # 그대로 반환\n","    finally:\n","        # 임시 wav 정리(영상 원본은 그대로 둠)\n","        try:\n","            os.remove(wav_path)\n","            os.rmdir(os.path.dirname(wav_path))\n","        except Exception:\n","            pass\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bbu87AoHm9us","executionInfo":{"status":"ok","timestamp":1758601865909,"user_tz":-540,"elapsed":7,"user":{"displayName":"김은지","userId":"00400322563815083189"}},"outputId":"36cdbf29-e5ac-4d9b-934c-120d23888c86"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing stt.py\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/Colab Notebooks/온라인해커톤"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xf7sJ369k0CF","executionInfo":{"status":"ok","timestamp":1758602061194,"user_tz":-540,"elapsed":46,"user":{"displayName":"김은지","userId":"00400322563815083189"}},"outputId":"b7d52633-56f6-4db4-ddc8-8160633a3fd1"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/온라인해커톤\n"]}]},{"cell_type":"code","source":["import os\n","with open('./key/openai_api_key', 'r') as f:\n","    api_key = f.read().strip()\n","\n","os.environ['OPENAI_API_KEY'] = api_key"],"metadata":{"id":"HBVI434hkz19","executionInfo":{"status":"ok","timestamp":1758602064117,"user_tz":-540,"elapsed":1798,"user":{"displayName":"김은지","userId":"00400322563815083189"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["from typing import Dict, List, Optional\n","from pydantic import BaseModel, Field\n","from typing import Literal\n","# UserProfile 클래스 정의\n","class UserProfile(BaseModel):\n","    mobility_issue: bool = Field(default=True, description=\"거동 불편 여부\")\n","    living_arrangement: Literal['alone', 'with_family'] = Field(default='alone', description=\"가족 동거 여부\")\n","    wake_up_time: str = Field(default=\"07:00\", description=\"기상 시간\")\n","    bed_time: str = Field(default=\"21:00\", description=\"취침 시간\")\n"],"metadata":{"id":"nwcGsWyzFGU6","executionInfo":{"status":"ok","timestamp":1758605911690,"user_tz":-540,"elapsed":4,"user":{"displayName":"김은지","userId":"00400322563815083189"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["# next_pipeline.py\n","import os\n","from typing import Dict, List\n","from openai import OpenAI\n","from stt import transcribe_video\n","from typing import Optional\n","import json\n","\n","\n","# 1) 키 로드\n","with open('./key/openai_api_key', 'r') as f:\n","    api_key = f.read().strip()\n","\n","# 2) 환경 변수 등록\n","os.environ['OPENAI_API_KEY'] = api_key\n","\n","# 3) 클라이언트 생성\n","client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n","\n","\n","# === 1) 텍스트 감정 ===\n","EMO_LABELS = [\"happy\",\"neutral\",\"sad\",\"angry\",\"fear\",\"disgust\",\"surprise\"]\n","\n","def classify_text_emotion(text: str) -> Dict:\n","    prompt = f\"\"\"\n","다음 한국어 텍스트의 감정을 분석하세요.\n","- 주 감정(primary): 가장 강하게 나타나는 감정 1개\n","- 보조 감정(secondary): 함께 감지되는 부가적인 감정 0~2개\n","  * 신뢰도 0.3 이상인 경우만 포함\n","  * 주 감정과 충분히 구별되는 경우만 포함\n","  * 없으면 빈 리스트\n","\n","가능한 감정: {\", \".join(EMO_LABELS)}\n","\n","JSON으로만 답하세요:\n","{{\n","  \"primary\": {{\"label\": \"감정명\", \"confidence\": 0.0~1.0}},\n","  \"secondary\": [\n","    {{\"label\": \"감정명\", \"confidence\": 0.0~1.0}},\n","    ...\n","  ]\n","}}\n","\n","텍스트: {text}\n","\"\"\"\n","    resp = client.chat.completions.create(\n","        model=\"gpt-4o-mini\",\n","        response_format={\"type\": \"json_object\"},\n","        temperature=0.0,\n","        messages=[\n","            {\"role\": \"system\", \"content\": \"You are a precise emotion classifier that detects multiple emotional layers.\"},\n","            {\"role\": \"user\", \"content\": prompt},\n","        ],\n","    )\n","    content = resp.choices[0].message.content\n","    data = json.loads(content)\n","\n","    # 응답 검증 및 기본값 설정\n","    if \"primary\" not in data:\n","        data[\"primary\"] = {\"label\": \"neutral\", \"confidence\": 0.5}\n","    if \"secondary\" not in data:\n","        data[\"secondary\"] = []\n","\n","    # 보조 감정 필터링 (신뢰도 낮은 것 제거)\n","    data[\"secondary\"] = [s for s in data[\"secondary\"] if s.get(\"confidence\", 0) >= 0.3][:2]\n","\n","    return data\n","\n","# === 2) 얼굴 감정 ===\n","def run_face_emotion(video_path: str) -> List[Dict]:\n","    \"\"\"\n","    TODO: 실제 얼굴/표정 모델 연결.\n","    지금은 빈 리스트 반환 (텍스트만 사용).\n","    \"\"\"\n","    return []  # 예: [{\"label\":\"happy\",\"confidence\":0.72}]\n","\n","# === 3) 융합 로직 (얼굴 우선, 근소 차이는 텍스트 허용) ===\n","def fuse_emotion(video_preds: List[Dict], text_pred: Dict,\n","                 min_conf=0.35, tiny_gap=0.05) -> Dict:\n","    \"\"\"\n","    영상과 텍스트 감정을 융합\n","    - 주 감정은 기존 로직 유지\n","    - 보조 감정은 텍스트에서만 가져옴\n","    \"\"\"\n","    # 기존 로직으로 주 감정 결정\n","    v_top = max(video_preds, key=lambda x: x[\"confidence\"]) if video_preds else None\n","    t_primary = text_pred.get(\"primary\", {\"label\": \"neutral\", \"confidence\": 0.5})\n","\n","    # 주 감정 선택\n","    if v_top and v_top[\"confidence\"] >= min_conf:\n","        if abs(v_top[\"confidence\"] - t_primary[\"confidence\"]) <= tiny_gap:\n","            primary = {\"label\": t_primary[\"label\"],\n","                      \"confidence\": round(t_primary[\"confidence\"], 2),\n","                      \"source\": \"text_narrow_gap\"}\n","        else:\n","            primary = {\"label\": v_top[\"label\"],\n","                      \"confidence\": round(v_top[\"confidence\"], 2),\n","                      \"source\": \"video\"}\n","    else:\n","        primary = {\"label\": t_primary[\"label\"],\n","                  \"confidence\": round(t_primary[\"confidence\"], 2),\n","                  \"source\": \"text\"}\n","\n","    # 보조 감정은 텍스트에서 가져오되, 주 감정과 중복 제거\n","    secondary = []\n","    for s in text_pred.get(\"secondary\", []):\n","        if s[\"label\"] != primary[\"label\"]:\n","            secondary.append({\n","                \"label\": s[\"label\"],\n","                \"confidence\": round(s[\"confidence\"], 2)\n","            })\n","\n","    return {\n","        \"primary\": primary,\n","        \"secondary\": secondary\n","    }\n","\n","# === 4) 공감 멘트 ===\n","def build_empathy(fused: Dict, text: str) -> str:\n","    # 감정 설명 문자열 생성\n","    emotion_desc = f\"주 감정: {fused['primary']['label']} (신뢰도 {fused['primary']['confidence']})\"\n","\n","    if fused.get('secondary'):\n","        secondary_labels = [f\"{s['label']}({s['confidence']})\" for s in fused['secondary']]\n","        emotion_desc += f\"\\n보조 감정: {', '.join(secondary_labels)}\"\n","\n","    prompt = f\"\"\"\n","아래 정보를 바탕으로 고령자에게 공감과 격려를 전하는 짧은 문단을 만드세요.\n","- 존댓말, 쉬운 어휘, 200자 이내\n","- 이모지/특수기호/괄호 금지\n","- 구조: 감사 → 감정요약 → 공감/격려 → 맞춤 제안\n","- 주 감정을 중심으로 하되, 보조 감정도 자연스럽게 언급\n","\n","{emotion_desc}\n","말씀 요지: {text[:120]}\n","\"\"\"\n","    resp = client.chat.completions.create(\n","        model=\"gpt-4o-mini\",\n","        temperature=0.3,\n","        messages=[\n","            {\"role\":\"system\",\"content\":\"You are a warm, emotionally perceptive Korean caregiver assistant.\"},\n","            {\"role\":\"user\",\"content\":prompt}\n","        ],\n","    )\n","    return resp.choices[0].message.content.strip()\n","\n","\n","# === 5) 감정 기반 미션 ===\n","def make_emo_mission(fused: Dict, text: str, user_profile: Optional[UserProfile] = None) -> Dict:\n","    \"\"\"감정 기반 미션 생성 (선택적으로 사용자 프로필 고려)\"\"\"\n","\n","    # 감정 설명\n","    emotion_desc = f\"주 감정: {fused['primary']['label']} (신뢰도 {fused['primary']['confidence']})\"\n","    if fused.get('secondary'):\n","        secondary_labels = [s['label'] for s in fused['secondary']]\n","        emotion_desc += f\"\\n보조 감정: {', '.join(secondary_labels)}\"\n","\n","    # 사용자 프로필이 있으면 추가\n","    user_context = \"\"\n","    if user_profile:\n","        user_context = f\"\"\"\n","\n","사용자 상황:\n","- 거동 불편 여부: {'예 (이동 제한, 앉아서 가능한 활동 위주)' if user_profile.mobility_issue else '아니오 (자유로운 이동 가능)'}\n","- 거주 형태: {'독거 (혼자서 안전하게 할 수 있는 활동)' if user_profile.living_arrangement == 'alone' else '가족과 함께 (가족 참여 가능)'}\n","\n","위 상황을 반드시 고려하여 적합한 미션을 제안하세요.\n","\"\"\"\n","\n","    prompt = f\"\"\"\n","고령자 친화 '감정 기반 미션' 1개를 만들어주세요.\n","- 존댓말, 쉬운 어휘, 이모지/특수기호/괄호 금지\n","- 의료·법률·위험 활동·개인정보 요구 금지\n","- 출력은 JSON만: title(12자 이내), steps(60자 이내), duration(예: 3분), difficulty(very_easy|easy)\n","{user_context}\n","{emotion_desc}\n","말씀 요지: {text[:120]}\n","\"\"\"\n","\n","    resp = client.chat.completions.create(\n","        model=\"gpt-4o-mini\",\n","        response_format={\"type\": \"json_object\"},\n","        temperature=0.3,\n","        messages=[\n","            {\"role\": \"system\", \"content\": \"You write short, safe, elder-friendly tasks in Korean.\"},\n","            {\"role\": \"user\", \"content\": prompt},\n","        ],\n","    )\n","    content = resp.choices[0].message.content\n","    data = json.loads(content)\n","    return data\n","\n","# === 6) 전체 실행 ===\n","# === 기존의 run_full_pipeline을 두 개로 분리 ===\n","\n","def run_core_pipeline(video_path: str) -> Dict:\n","    \"\"\"\n","    핵심 파이프라인 (미션 제외)\n","    STT → 감정 분석 → 융합 → 공감 멘트\n","    \"\"\"\n","    # 1) STT\n","    text = transcribe_video(video_path)\n","\n","    # 2) 감정들\n","    txt_pred = classify_text_emotion(text)  # {\"primary\": {...}, \"secondary\": [...]}\n","    vid_preds = run_face_emotion(video_path)  # [] (추후 구현)\n","\n","    # 3) 융합\n","    fused = fuse_emotion(vid_preds, txt_pred)\n","\n","    # 4) 공감 멘트만 생성\n","    empathy = build_empathy(fused, text)\n","\n","    return {\n","        \"stt_text\": text,\n","        \"text_emotion\": txt_pred,\n","        \"video_emotions\": vid_preds,\n","        \"fused_emotion\": fused,\n","        \"empathy\": empathy\n","        # \"mission\" 없음!\n","    }\n","\n","\n","def generate_emo_mission(core_result: Dict, user_profile: Optional[UserProfile] = None) -> Dict:\n","    \"\"\"\n","    미션 생성 (사용자 프로필 선택적 고려)\n","    \"\"\"\n","    mission = make_emo_mission(\n","        core_result[\"fused_emotion\"],\n","        core_result[\"stt_text\"],\n","        user_profile  # 그대로 전달\n","    )\n","    return mission\n","\n"],"metadata":{"id":"SeHpmqg0zJ6B","executionInfo":{"status":"ok","timestamp":1758605982519,"user_tz":-540,"elapsed":60,"user":{"displayName":"김은지","userId":"00400322563815083189"}}},"execution_count":29,"outputs":[]}]}