#!/usr/bin/env python3
"""
FER2013 ê°ì • ì¸ì‹ ëª¨ë¸ í†µí•© ì‹¤í–‰ ì½”ë“œ
í—¬ìŠ¤ì¼€ì–´ í•´ì»¤í†¤ - ê°ì • ë¶„ì„ ëª¨ë¸

ì´ íŒŒì¼ì€ fer2013-finetuningì˜ ëª¨ë“  ëª¨ë“ˆì„ í†µí•©í•˜ì—¬
ë‹¨ì¼ íŒŒì¼ë¡œ ì‹¤í–‰ ê°€ëŠ¥í•˜ë„ë¡ êµ¬ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.

ë˜í•œ ê° ê¸°ëŠ¥ë“¤ì„ ìƒì„¸íˆ ì„¤ëª…í•˜ì—¬ ì½”ë“œ ì´í•´ë¥¼ ë•ê¸° ìœ„í•œ íŒŒì¼ì…ë‹ˆë‹¤.

FER2013 (Facial Expression Recognition 2013) ë°ì´í„°ì…‹:
- Kaggleì—ì„œ ì œê³µí•˜ëŠ” ì–¼êµ´ ê°ì • ì¸ì‹ ë°ì´í„°ì…‹
- 7ê°€ì§€ ê°ì • í´ë˜ìŠ¤: í™”ë‚¨, í˜ì˜¤, ë‘ë ¤ì›€, í–‰ë³µ, ì¤‘ë¦½, ìŠ¬í””, ë†€ëŒ
- 48x48 í”½ì…€ ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ì´ë¯¸ì§€ë¡œ êµ¬ì„±
"""

# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
import os                    # íŒŒì¼ ì‹œìŠ¤í…œ ì‘ì—…ìš©
import sys                   # ì‹œìŠ¤í…œ ê´€ë ¨ íŒŒë¼ë¯¸í„°ì™€ í•¨ìˆ˜
import torch                 # PyTorch ë©”ì¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
import torch.nn as nn        # ì‹ ê²½ë§ ë ˆì´ì–´ì™€ í•¨ìˆ˜ë“¤
import torch.optim as optim  # ìµœì í™” ì•Œê³ ë¦¬ì¦˜ë“¤ (Adam, SGD ë“±)
import timm                  # PyTorch Image Models - ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
from torch.utils.data import Dataset, DataLoader  # ë°ì´í„° ë¡œë”© ìœ í‹¸ë¦¬í‹°
from torchvision import transforms                 # ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ë³€í™˜ í•¨ìˆ˜ë“¤
from PIL import Image        # Python Imaging Library - ì´ë¯¸ì§€ ì²˜ë¦¬
from tqdm import tqdm        # ì§„í–‰ë¥  í‘œì‹œ ë°”
import matplotlib.pyplot as plt  # ê·¸ë˜í”„ ì‹œê°í™”

# ===============================
# Configuration - ëª¨ë¸ í•™ìŠµì— í•„ìš”í•œ ëª¨ë“  ì„¤ì •ê°’ë“¤
# ===============================
class Config:
    """
    ëª¨ë¸ ë° í•™ìŠµ ì„¤ì • í´ë˜ìŠ¤
    ëª¨ë“  í•˜ì´í¼íŒŒë¼ë¯¸í„°ì™€ ì„¤ì •ê°’ì„ í•œ ê³³ì—ì„œ ê´€ë¦¬
    """
    
    # ========== ë°ì´í„° ê´€ë ¨ ì„¤ì • ==========
    DATA_DIR = './fer2013'  # FER2013 ë°ì´í„°ì…‹ì´ ì €ì¥ëœ ë””ë ‰í† ë¦¬ ê²½ë¡œ
    BATCH_SIZE = 256        # í•œ ë²ˆì— ì²˜ë¦¬í•  ì´ë¯¸ì§€ ê°œìˆ˜ (GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì •)
    NUM_WORKERS = 4         # ë°ì´í„° ë¡œë”©ì— ì‚¬ìš©í•  CPU í”„ë¡œì„¸ìŠ¤ ê°œìˆ˜
    
    # ========== ëª¨ë¸ ê´€ë ¨ ì„¤ì • ==========
    # MobileNetV4 ëª¨ë¸ëª… í•´ì„:
    # - hf_hub: Hugging Face Hubì—ì„œ ì œê³µí•˜ëŠ” ëª¨ë¸
    # - timm: PyTorch Image Models ë¼ì´ë¸ŒëŸ¬ë¦¬
    # - mobilenetv4: Googleì˜ MobileNet ì•„í‚¤í…ì²˜ 4ë²ˆì§¸ ë²„ì „
    # - conv_medium: ì¤‘ê°„ í¬ê¸°ì˜ ì»¨ë³¼ë£¨ì…˜ ë²„ì „ (small < medium < large)
    # - e500: 500 ì—í¬í¬ ë™ì•ˆ ì‚¬ì „ í›ˆë ¨ë¨
    # - r224: ì…ë ¥ í•´ìƒë„ 224x224 í”½ì…€
    # - in1k: ImageNet-1K ë°ì´í„°ì…‹ìœ¼ë¡œ ì‚¬ì „ í›ˆë ¨ë¨ (1000ê°œ í´ë˜ìŠ¤)
    MODEL_NAME = "hf_hub:timm/mobilenetv4_conv_medium.e500_r224_in1k"
    NUM_CLASSES = 7         # FER2013ì˜ ê°ì • í´ë˜ìŠ¤ ê°œìˆ˜ (7ê°€ì§€ ê°ì •)
    
    # ========== í•™ìŠµ ê´€ë ¨ ì„¤ì • ==========
    # GPUê°€ ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ GPUë¥¼, ì—†ìœ¼ë©´ CPUë¥¼ ì‚¬ìš©
    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # ========== 1ë‹¨ê³„: ë¶„ë¥˜ê¸° í•™ìŠµ ì„¤ì • ==========
    # Transfer Learningì˜ ì²« ë²ˆì§¸ ë‹¨ê³„
    # ì‚¬ì „ í›ˆë ¨ëœ ë°±ë³¸ì€ ê³ ì •í•˜ê³  ìƒˆë¡œìš´ ë¶„ë¥˜ê¸°ë§Œ í•™ìŠµ
    CLASSIFIER_EPOCHS = 10  # ë¶„ë¥˜ê¸° í•™ìŠµ ì—í¬í¬ ìˆ˜
    CLASSIFIER_LR = 1e-3    # ë¶„ë¥˜ê¸° í•™ìŠµë¥  (0.001)
    
    # ========== 2ë‹¨ê³„: ì „ì²´ ë„¤íŠ¸ì›Œí¬ ë¯¸ì„¸ì¡°ì • ì„¤ì • ==========
    # ì „ì²´ ë„¤íŠ¸ì›Œí¬ë¥¼ ë¯¸ì„¸í•˜ê²Œ ì¡°ì •í•˜ëŠ” ë‹¨ê³„
    FULL_NETWORK_EPOCHS = 15    # ì „ì²´ ë„¤íŠ¸ì›Œí¬ í•™ìŠµ ì—í¬í¬ ìˆ˜
    BACKBONE_LR = 1e-5          # ë°±ë³¸ ë„¤íŠ¸ì›Œí¬ í•™ìŠµë¥  (0.00001) - ë§¤ìš° ì‘ê²Œ ì„¤ì •
    CLASSIFIER_LR_FINETUNE = 1e-4  # ë¶„ë¥˜ê¸° ë¯¸ì„¸ì¡°ì • í•™ìŠµë¥  (0.0001)

# ===============================
# Model Architecture - ê°ì • ë¶„ë¥˜ ëª¨ë¸ êµ¬ì¡° ì •ì˜
# ===============================
def create_emotion_classifier(num_classes=7, backbone_name="hf_hub:timm/mobilenetv4_conv_medium.e500_r224_in1k"):
    """
    ê°ì • ë¶„ë¥˜ë¥¼ ìœ„í•œ ë”¥ëŸ¬ë‹ ëª¨ë¸ ìƒì„± í•¨ìˆ˜
    
    Transfer Learning ë°©ì‹ì„ ì‚¬ìš©:
    1. ImageNetìœ¼ë¡œ ì‚¬ì „ í›ˆë ¨ëœ MobileNetV4 ë°±ë³¸ ì‚¬ìš©
    2. ë§ˆì§€ë§‰ ë¶„ë¥˜ê¸° ë ˆì´ì–´ë¥¼ 7ê°œ ê°ì • í´ë˜ìŠ¤ìš©ìœ¼ë¡œ êµì²´
    
    Args:
        num_classes (int): ë¶„ë¥˜í•  ê°ì • í´ë˜ìŠ¤ ê°œìˆ˜ (ê¸°ë³¸ê°’: 7)
        backbone_name (str): ì‚¬ìš©í•  ë°±ë³¸ ëª¨ë¸ëª…
    
    Returns:
        torch.nn.Module: ê°ì • ë¶„ë¥˜ìš©ìœ¼ë¡œ ìˆ˜ì •ëœ ëª¨ë¸
    """
    
    # timm ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ì‚¬ì „ í›ˆë ¨ëœ MobileNetV4 ëª¨ë¸ ë¡œë“œ
    # pretrained=True: ImageNetìœ¼ë¡œ ì‚¬ì „ í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ ì‚¬ìš©
    model = timm.create_model(backbone_name, pretrained=True)
    
    # ê¸°ì¡´ ë¶„ë¥˜ê¸°ì˜ ì…ë ¥ íŠ¹ì„± ìˆ˜ í™•ì¸ (MobileNetV4ì˜ ê²½ìš° 1280ê°œ)
    in_features = model.classifier.in_features
    
    # ê¸°ì¡´ ë¶„ë¥˜ê¸°ë¥¼ ìƒˆë¡œìš´ 3ì¸µ ë¶„ë¥˜ê¸°ë¡œ êµì²´
    # êµ¬ì¡°: 1280 â†’ 512 â†’ 128 â†’ 7 (ê°ì • í´ë˜ìŠ¤)
    model.classifier = nn.Sequential(
        # ì²« ë²ˆì§¸ ì¸µ: íŠ¹ì„± ì°¨ì› ì¶•ì†Œ (1280 â†’ 512)
        nn.Linear(in_features, 512),    # ì™„ì „ ì—°ê²°ì¸µ (Fully Connected)
        nn.BatchNorm1d(512),            # ë°°ì¹˜ ì •ê·œí™” (í•™ìŠµ ì•ˆì •í™”)
        nn.ReLU(),                      # ReLU í™œì„±í™” í•¨ìˆ˜ (ë¹„ì„ í˜•ì„± ì¶”ê°€)
        nn.Dropout(0.5),                # ë“œë¡­ì•„ì›ƒ 50% (ê³¼ì í•© ë°©ì§€)
        
        # ë‘ ë²ˆì§¸ ì¸µ: ì¶”ê°€ íŠ¹ì„± ì¶•ì†Œ (512 â†’ 128)
        nn.Linear(512, 128),            # ì™„ì „ ì—°ê²°ì¸µ
        nn.BatchNorm1d(128),            # ë°°ì¹˜ ì •ê·œí™”
        nn.ReLU(),                      # ReLU í™œì„±í™” í•¨ìˆ˜
        nn.Dropout(0.5),                # ë“œë¡­ì•„ì›ƒ 50%
        
        # ì¶œë ¥ ì¸µ: ìµœì¢… ë¶„ë¥˜ (128 â†’ 7ê°œ ê°ì • í´ë˜ìŠ¤)
        nn.Linear(128, num_classes)     # ìµœì¢… ë¶„ë¥˜ë¥¼ ìœ„í•œ ì™„ì „ ì—°ê²°ì¸µ
        # ì°¸ê³ : ë§ˆì§€ë§‰ ì¸µì—ëŠ” í™œì„±í™” í•¨ìˆ˜ ì—†ìŒ (CrossEntropyLossì—ì„œ ì²˜ë¦¬)
    )
    
    return model

# ===============================
# Dataset and DataLoader - ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬
# ===============================
class FERDataset(Dataset):
    """
    FER2013 ê°ì • ì¸ì‹ ë°ì´í„°ì…‹ í´ë˜ìŠ¤
    PyTorchì˜ Dataset í´ë˜ìŠ¤ë¥¼ ìƒì†ë°›ì•„ ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ êµ¬í˜„
    
    ë°ì´í„°ì…‹ êµ¬ì¡°:
    fer2013/
    â”œâ”€â”€ train/          # í•™ìŠµìš© ë°ì´í„°
    â”‚   â”œâ”€â”€ angry/      # í™”ë‚œ í‘œì • ì´ë¯¸ì§€ë“¤
    â”‚   â”œâ”€â”€ disgust/    # í˜ì˜¤ í‘œì • ì´ë¯¸ì§€ë“¤
    â”‚   â”œâ”€â”€ fear/       # ë‘ë ¤ìš´ í‘œì • ì´ë¯¸ì§€ë“¤
    â”‚   â”œâ”€â”€ happy/      # í–‰ë³µí•œ í‘œì • ì´ë¯¸ì§€ë“¤
    â”‚   â”œâ”€â”€ neutral/    # ì¤‘ë¦½ í‘œì • ì´ë¯¸ì§€ë“¤
    â”‚   â”œâ”€â”€ sad/        # ìŠ¬í”ˆ í‘œì • ì´ë¯¸ì§€ë“¤
    â”‚   â””â”€â”€ surprise/   # ë†€ë€ í‘œì • ì´ë¯¸ì§€ë“¤
    â””â”€â”€ test/           # ê²€ì¦ìš© ë°ì´í„° (ê°™ì€ êµ¬ì¡°)
    """
    
    def __init__(self, root_dir, is_train=True, transform=None):
        """
        ë°ì´í„°ì…‹ ì´ˆê¸°í™”
        
        Args:
            root_dir (str): ë°ì´í„°ì…‹ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ
            is_train (bool): Trueë©´ í•™ìŠµìš©, Falseë©´ ê²€ì¦ìš© ë°ì´í„°
            transform (callable): ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ë³€í™˜ í•¨ìˆ˜
        """
        self.root_dir = root_dir
        self.is_train = is_train
        self.transform = transform
        
        # í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ ì„¤ì •
        data_dir = os.path.join(root_dir, 'train' if is_train else 'test')
        
        # ì´ë¯¸ì§€ ê²½ë¡œì™€ ë ˆì´ë¸”ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
        self.image_paths = []
        self.labels = []
        
        # ê°ì • í´ë˜ìŠ¤ëª…ì„ ìˆ«ì ë ˆì´ë¸”ë¡œ ë§¤í•‘í•˜ëŠ” ë”•ì…”ë„ˆë¦¬
        # ë”¥ëŸ¬ë‹ ëª¨ë¸ì€ ìˆ«ì ë ˆì´ë¸”ë¡œ í•™ìŠµí•˜ë¯€ë¡œ ë¬¸ìì—´ì„ ìˆ«ìë¡œ ë³€í™˜
        self.emotion_map = {
            'angry': 0,     # í™”ë‚¨
            'disgust': 1,   # í˜ì˜¤
            'fear': 2,      # ë‘ë ¤ì›€
            'happy': 3,     # í–‰ë³µ
            'neutral': 4,   # ì¤‘ë¦½
            'sad': 5,       # ìŠ¬í””
            'surprise': 6   # ë†€ëŒ
        }
        
        # ê° ê°ì • í´ë”ì—ì„œ ì´ë¯¸ì§€ íŒŒì¼ë“¤ì„ ì°¾ì•„ì„œ ê²½ë¡œì™€ ë ˆì´ë¸” ìˆ˜ì§‘
        for emotion in self.emotion_map.keys():
            emotion_dir = os.path.join(data_dir, emotion)
            if os.path.exists(emotion_dir):
                # í•´ë‹¹ ê°ì • í´ë”ì˜ ëª¨ë“  ì´ë¯¸ì§€ íŒŒì¼ ì²˜ë¦¬
                for img_name in os.listdir(emotion_dir):
                    # ì´ë¯¸ì§€ íŒŒì¼ í™•ì¥ì í™•ì¸ (jpg, jpeg, pngë§Œ ì²˜ë¦¬)
                    if img_name.endswith(('.jpg', '.jpeg', '.png')):
                        # ì´ë¯¸ì§€ ì „ì²´ ê²½ë¡œ ì €ì¥
                        self.image_paths.append(os.path.join(emotion_dir, img_name))
                        # í•´ë‹¹ ê°ì •ì˜ ìˆ«ì ë ˆì´ë¸” ì €ì¥
                        self.labels.append(self.emotion_map[emotion])
    
    def __len__(self):
        """
        ë°ì´í„°ì…‹ì˜ ì´ ìƒ˜í”Œ ê°œìˆ˜ ë°˜í™˜
        PyTorch DataLoaderì—ì„œ ìë™ìœ¼ë¡œ í˜¸ì¶œë¨
        """
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        """
        ì£¼ì–´ì§„ ì¸ë±ìŠ¤ì˜ ë°ì´í„° ìƒ˜í”Œ ë°˜í™˜
        PyTorch DataLoaderì—ì„œ ë°°ì¹˜ ìƒì„± ì‹œ ìë™ìœ¼ë¡œ í˜¸ì¶œë¨
        
        Args:
            idx (int): ê°€ì ¸ì˜¬ ë°ì´í„°ì˜ ì¸ë±ìŠ¤
            
        Returns:
            tuple: (ì´ë¯¸ì§€ í…ì„œ, ê°ì • ë ˆì´ë¸”) ìŒ
        """
        # ì¸ë±ìŠ¤ì— í•´ë‹¹í•˜ëŠ” ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
        img_path = self.image_paths[idx]
        
        # PILì„ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ ë¡œë“œ ë° RGB í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        # convert('RGB'): ê·¸ë ˆì´ìŠ¤ì¼€ì¼ì´ë‚˜ ë‹¤ë¥¸ í˜•ì‹ì„ RGBë¡œ í†µì¼
        image = Image.open(img_path).convert('RGB')
        
        # í•´ë‹¹ ì´ë¯¸ì§€ì˜ ê°ì • ë ˆì´ë¸” ê°€ì ¸ì˜¤ê¸°
        label = self.labels[idx]
        
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ë³€í™˜ ì ìš© (ìˆëŠ” ê²½ìš°)
        # transformì—ëŠ” í¬ê¸° ì¡°ì •, ì •ê·œí™”, ë°ì´í„° ì¦ê°• ë“±ì´ í¬í•¨ë¨
        if self.transform:
            image = self.transform(image)
        
        return image, label

def get_transforms(is_train=True):
    """
    ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ë³€í™˜ í•¨ìˆ˜ë“¤ì„ ì •ì˜
    
    Args:
        is_train (bool): Trueë©´ í•™ìŠµìš© ë³€í™˜, Falseë©´ ê²€ì¦ìš© ë³€í™˜
        
    Returns:
        transforms.Compose: ì „ì²˜ë¦¬ ë³€í™˜ë“¤ì˜ ì¡°í•©
    """
    if is_train:
        # í•™ìŠµìš© ë³€í™˜: ë°ì´í„° ì¦ê°•(Data Augmentation) í¬í•¨
        return transforms.Compose([
            # 1. ì´ë¯¸ì§€ í¬ê¸°ë¥¼ 224x224ë¡œ ì¡°ì • (MobileNetV4 ì…ë ¥ í¬ê¸°)
            transforms.Resize((224, 224)),
            
            # 2. ë°ì´í„° ì¦ê°• ê¸°ë²•ë“¤ (í•™ìŠµ ë°ì´í„° ë‹¤ì–‘ì„± ì¦ê°€)
            transforms.RandomHorizontalFlip(),  # 50% í™•ë¥ ë¡œ ì¢Œìš° ë°˜ì „
            transforms.RandomRotation(10),      # Â±10ë„ ë²”ìœ„ì—ì„œ ë¬´ì‘ìœ„ íšŒì „
            transforms.ColorJitter(             # ìƒ‰ìƒ, ë°ê¸° ë¬´ì‘ìœ„ ë³€ê²½
                brightness=0.2,  # ë°ê¸° Â±20% ë³€ê²½
                contrast=0.2     # ëŒ€ë¹„ Â±20% ë³€ê²½
            ),
            
            # 3. PIL ì´ë¯¸ì§€ë¥¼ PyTorch í…ì„œë¡œ ë³€í™˜ (0-255 â†’ 0-1 ì •ê·œí™”)
            transforms.ToTensor(),
            
            # 4. ImageNet í†µê³„ê°’ìœ¼ë¡œ ì •ê·œí™” (ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ê³¼ ì¼ì¹˜ì‹œí‚¤ê¸° ìœ„í•´)
            # RGB ê° ì±„ë„ë³„ í‰ê· ê³¼ í‘œì¤€í¸ì°¨
            transforms.Normalize(
                mean=[0.485, 0.456, 0.406],  # ImageNet RGB í‰ê· ê°’
                std=[0.229, 0.224, 0.225]    # ImageNet RGB í‘œì¤€í¸ì°¨
            )
        ])
    else:
        # ê²€ì¦ìš© ë³€í™˜: ë°ì´í„° ì¦ê°• ì—†ì´ ê¸°ë³¸ ì „ì²˜ë¦¬ë§Œ
        return transforms.Compose([
            # 1. ì´ë¯¸ì§€ í¬ê¸° ì¡°ì •
            transforms.Resize((224, 224)),
            
            # 2. í…ì„œ ë³€í™˜
            transforms.ToTensor(),
            
            # 3. ì •ê·œí™” (í•™ìŠµìš©ê³¼ ë™ì¼)
            transforms.Normalize(
                mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225]
            )
        ])

def create_dataloaders(data_dir, batch_size=32, num_workers=4):
    """
    í•™ìŠµìš©ê³¼ ê²€ì¦ìš© ë°ì´í„°ë¡œë” ìƒì„±
    
    Args:
        data_dir (str): ë°ì´í„°ì…‹ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        batch_size (int): ë°°ì¹˜ í¬ê¸° (í•œ ë²ˆì— ì²˜ë¦¬í•  ì´ë¯¸ì§€ ìˆ˜)
        num_workers (int): ë°ì´í„° ë¡œë”©ì— ì‚¬ìš©í•  CPU í”„ë¡œì„¸ìŠ¤ ìˆ˜
        
    Returns:
        tuple: (í•™ìŠµìš© DataLoader, ê²€ì¦ìš© DataLoader)
    """
    
    # í•™ìŠµìš© ë°ì´í„°ì…‹ ìƒì„± (ë°ì´í„° ì¦ê°• í¬í•¨)
    train_dataset = FERDataset(
        root_dir=data_dir,
        is_train=True,                          # í•™ìŠµ ëª¨ë“œ
        transform=get_transforms(is_train=True) # ë°ì´í„° ì¦ê°• í¬í•¨ëœ ë³€í™˜
    )
    
    # ê²€ì¦ìš© ë°ì´í„°ì…‹ ìƒì„± (ë°ì´í„° ì¦ê°• ì—†ìŒ)
    val_dataset = FERDataset(
        root_dir=data_dir,
        is_train=False,                          # ê²€ì¦ ëª¨ë“œ
        transform=get_transforms(is_train=False) # ê¸°ë³¸ ë³€í™˜ë§Œ
    )
    
    # í•™ìŠµìš© ë°ì´í„°ë¡œë” ìƒì„±
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,  # ë°°ì¹˜ í¬ê¸°
        shuffle=True,           # í•™ìŠµ ì‹œ ë°ì´í„° ìˆœì„œë¥¼ ë¬´ì‘ìœ„ë¡œ ì„ìŒ (ì¤‘ìš”!)
        num_workers=num_workers,# ë©€í‹°í”„ë¡œì„¸ì‹±ìœ¼ë¡œ ë°ì´í„° ë¡œë”© ì†ë„ í–¥ìƒ
        pin_memory=True         # GPU ì „ì†¡ ì†ë„ í–¥ìƒ (CUDA ì‚¬ìš© ì‹œ)
    )
    
    # ê²€ì¦ìš© ë°ì´í„°ë¡œë” ìƒì„±
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,  # ë°°ì¹˜ í¬ê¸°
        shuffle=False,          # ê²€ì¦ ì‹œì—ëŠ” ìˆœì„œ ìœ ì§€ (ì¬í˜„ì„± ìœ„í•´)
        num_workers=num_workers,# ë©€í‹°í”„ë¡œì„¸ì‹±
        pin_memory=True         # GPU ì „ì†¡ ì†ë„ í–¥ìƒ
    )
    
    return train_loader, val_loader

# ===============================
# Training Logic - ëª¨ë¸ í•™ìŠµ ë¡œì§
# ===============================
class EmotionTrainer:
    """
    ê°ì • ì¸ì‹ ëª¨ë¸ í•™ìŠµì„ ë‹´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤
    2ë‹¨ê³„ Transfer Learning ë°©ì‹ìœ¼ë¡œ í•™ìŠµ ì§„í–‰:
    1ë‹¨ê³„: ë¶„ë¥˜ê¸°ë§Œ í•™ìŠµ (ë°±ë³¸ ê³ ì •)
    2ë‹¨ê³„: ì „ì²´ ë„¤íŠ¸ì›Œí¬ ë¯¸ì„¸ì¡°ì •
    """
    
    def __init__(self, model, train_loader, val_loader, device='cuda'):
        """
        íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”
        
        Args:
            model: í•™ìŠµí•  ëª¨ë¸
            train_loader: í•™ìŠµìš© ë°ì´í„°ë¡œë”
            val_loader: ê²€ì¦ìš© ë°ì´í„°ë¡œë”
            device: í•™ìŠµì— ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ ('cuda' ë˜ëŠ” 'cpu')
        """
        self.model = model.to(device)  # ëª¨ë¸ì„ ì§€ì •ëœ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.device = device
        
    def train_classifier_only(self, epochs=10, lr=1e-3):
        """
        1ë‹¨ê³„: ë¶„ë¥˜ê¸° ë ˆì´ì–´ë§Œ í•™ìŠµ (ë°±ë³¸ ë„¤íŠ¸ì›Œí¬ëŠ” ê³ ì •)
        
        Transfer Learningì˜ ì²« ë²ˆì§¸ ë‹¨ê³„ë¡œ, ì‚¬ì „ í›ˆë ¨ëœ íŠ¹ì„± ì¶”ì¶œê¸°ëŠ” 
        ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ê³  ìƒˆë¡œìš´ ì‘ì—…(ê°ì • ë¶„ë¥˜)ì— ë§ëŠ” ë¶„ë¥˜ê¸°ë§Œ í•™ìŠµ
        
        Args:
            epochs (int): í•™ìŠµ ì—í¬í¬ ìˆ˜
            lr (float): í•™ìŠµë¥ 
            
        Returns:
            dict: í•™ìŠµ íˆìŠ¤í† ë¦¬ (ì†ì‹¤, ì •í™•ë„ ê¸°ë¡)
        """
        
        # ë°±ë³¸ ë„¤íŠ¸ì›Œí¬ì˜ ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ ê³ ì • (gradient ê³„ì‚° ì•ˆí•¨)
        for name, param in self.model.named_parameters():
            if 'classifier' not in name:  # ë¶„ë¥˜ê¸°ê°€ ì•„ë‹Œ ëª¨ë“  ë ˆì´ì–´
                param.requires_grad = False  # ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ ë¹„í™œì„±í™”
        
        # ì†ì‹¤ í•¨ìˆ˜: ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ìš© CrossEntropy
        criterion = nn.CrossEntropyLoss()
        
        # ìµœì í™” ì•Œê³ ë¦¬ì¦˜: Adam (ë¶„ë¥˜ê¸° íŒŒë¼ë¯¸í„°ë§Œ ëŒ€ìƒ)
        optimizer = optim.Adam(self.model.classifier.parameters(), lr=lr)
        
        # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬: 5 ì—í¬í¬ë§ˆë‹¤ í•™ìŠµë¥ ì„ 50% ê°ì†Œ
        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)
        
        return self._train(criterion, optimizer, scheduler, epochs)
    
    def train_full_network(self, epochs=15, backbone_lr=1e-5, classifier_lr=1e-4):
        """
        2ë‹¨ê³„: ì „ì²´ ë„¤íŠ¸ì›Œí¬ ë¯¸ì„¸ì¡°ì • (Fine-tuning)
        
        1ë‹¨ê³„ì—ì„œ í•™ìŠµëœ ë¶„ë¥˜ê¸°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì „ì²´ ë„¤íŠ¸ì›Œí¬ë¥¼ ë¯¸ì„¸í•˜ê²Œ ì¡°ì •
        ë°±ë³¸ê³¼ ë¶„ë¥˜ê¸°ì— ì„œë¡œ ë‹¤ë¥¸ í•™ìŠµë¥ ì„ ì ìš©í•˜ì—¬ ì•ˆì •ì ì¸ í•™ìŠµ ì§„í–‰
        
        Args:
            epochs (int): í•™ìŠµ ì—í¬í¬ ìˆ˜
            backbone_lr (float): ë°±ë³¸ ë„¤íŠ¸ì›Œí¬ í•™ìŠµë¥  (ì‘ê²Œ ì„¤ì •)
            classifier_lr (float): ë¶„ë¥˜ê¸° í•™ìŠµë¥  (ìƒëŒ€ì ìœ¼ë¡œ í¬ê²Œ ì„¤ì •)
            
        Returns:
            dict: í•™ìŠµ íˆìŠ¤í† ë¦¬ (ì†ì‹¤, ì •í™•ë„ ê¸°ë¡)
        """
        
        # ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ í•™ìŠµ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ì •
        for param in self.model.parameters():
            param.requires_grad = True
            
        # ë°±ë³¸ê³¼ ë¶„ë¥˜ê¸° íŒŒë¼ë¯¸í„°ë¥¼ ë¶„ë¦¬í•˜ì—¬ ë‹¤ë¥¸ í•™ìŠµë¥  ì ìš©
        backbone_params = []    # ë°±ë³¸ ë„¤íŠ¸ì›Œí¬ íŒŒë¼ë¯¸í„°ë“¤
        classifier_params = []  # ë¶„ë¥˜ê¸° íŒŒë¼ë¯¸í„°ë“¤
        
        for name, param in self.model.named_parameters():
            if 'classifier' in name:
                classifier_params.append(param)
            else:
                backbone_params.append(param)
        
        # ì°¨ë³„ì  í•™ìŠµë¥ ì„ ê°€ì§„ ì˜µí‹°ë§ˆì´ì € ì„¤ì •
        # ë°±ë³¸ì€ ì´ë¯¸ ì¢‹ì€ íŠ¹ì„±ì„ í•™ìŠµí–ˆìœ¼ë¯€ë¡œ ì‘ì€ í•™ìŠµë¥  ì‚¬ìš©
        # ë¶„ë¥˜ê¸°ëŠ” ìƒˆë¡œ í•™ìŠµí•˜ë¯€ë¡œ ìƒëŒ€ì ìœ¼ë¡œ í° í•™ìŠµë¥  ì‚¬ìš©
        optimizer = optim.Adam([
            {'params': backbone_params, 'lr': backbone_lr},      # ë°±ë³¸: ì‘ì€ í•™ìŠµë¥ 
            {'params': classifier_params, 'lr': classifier_lr}   # ë¶„ë¥˜ê¸°: í° í•™ìŠµë¥ 
        ])
        
        # ì†ì‹¤ í•¨ìˆ˜
        criterion = nn.CrossEntropyLoss()
        
        # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬: 7 ì—í¬í¬ë§ˆë‹¤ í•™ìŠµë¥ ì„ 30% ê°ì†Œ
        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.3)
        
        return self._train(criterion, optimizer, scheduler, epochs)
    
    def _train(self, criterion, optimizer, scheduler, epochs):
        """
        ì‹¤ì œ í•™ìŠµ ë£¨í”„ë¥¼ ì‹¤í–‰í•˜ëŠ” ë‚´ë¶€ ë©”ì„œë“œ
        
        Args:
            criterion: ì†ì‹¤ í•¨ìˆ˜
            optimizer: ìµœì í™” ì•Œê³ ë¦¬ì¦˜
            scheduler: í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬
            epochs: í•™ìŠµí•  ì—í¬í¬ ìˆ˜
            
        Returns:
            dict: í•™ìŠµ íˆìŠ¤í† ë¦¬ (ê° ì—í¬í¬ë³„ ì†ì‹¤ê³¼ ì •í™•ë„ ê¸°ë¡)
        """
        
        best_val_acc = 0.0  # ìµœê³  ê²€ì¦ ì •í™•ë„ ì¶”ì ìš©
        
        # í•™ìŠµ ê¸°ë¡ì„ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬
        history = {
            'train_loss': [],  # í•™ìŠµ ì†ì‹¤ ê¸°ë¡
            'train_acc': [],   # í•™ìŠµ ì •í™•ë„ ê¸°ë¡
            'val_loss': [],    # ê²€ì¦ ì†ì‹¤ ê¸°ë¡
            'val_acc': []      # ê²€ì¦ ì •í™•ë„ ê¸°ë¡
        }
        
        # ê° ì—í¬í¬ë³„ í•™ìŠµ ì§„í–‰
        for epoch in range(epochs):
            
            # ========== í›ˆë ¨ ë‹¨ê³„ ==========
            self.model.train()  # ëª¨ë¸ì„ í•™ìŠµ ëª¨ë“œë¡œ ì„¤ì • (dropout, batchnorm í™œì„±í™”)
            train_metrics = self._train_epoch(criterion, optimizer)
            
            # ========== ê²€ì¦ ë‹¨ê³„ ==========
            self.model.eval()   # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì • (dropout, batchnorm ë¹„í™œì„±í™”)
            val_metrics = self._validate_epoch(criterion)
            
            # ========== ë©”íŠ¸ë¦­ ê¸°ë¡ ==========
            # í•™ìŠµ ë©”íŠ¸ë¦­ ì €ì¥
            for k, v in train_metrics.items():
                history[f'train_{k}'].append(v)
            # ê²€ì¦ ë©”íŠ¸ë¦­ ì €ì¥
            for k, v in val_metrics.items():
                history[f'val_{k}'].append(v)
            
            # ========== ì§„í–‰ ìƒí™© ì¶œë ¥ ==========
            print(f'Epoch {epoch+1}/{epochs}:')
            print(f"  Train Loss: {train_metrics['loss']:.4f}, Train Acc: {train_metrics['acc']:.2f}%")
            print(f"  Val Loss: {val_metrics['loss']:.4f}, Val Acc: {val_metrics['acc']:.2f}%")
            
            # ========== ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥ ==========
            # ê²€ì¦ ì •í™•ë„ê°€ ì´ì „ ìµœê³  ê¸°ë¡ì„ ê°±ì‹ í•˜ë©´ ëª¨ë¸ ì €ì¥
            if val_metrics['acc'] > best_val_acc:
                best_val_acc = val_metrics['acc']
                torch.save(self.model.state_dict(), 'best_model.pth')  # ëª¨ë¸ ê°€ì¤‘ì¹˜ ì €ì¥
                print(f"  ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥! ëª¨ë¸ ì €ì¥ë¨ (Val Acc: {val_metrics['acc']:.2f}%)")
            
            # ========== í•™ìŠµë¥  ì—…ë°ì´íŠ¸ ==========
            scheduler.step()  # ìŠ¤ì¼€ì¤„ëŸ¬ì— ë”°ë¼ í•™ìŠµë¥  ì¡°ì •
            print()
        
        return history
    
    def _train_epoch(self, criterion, optimizer):
        """
        í•œ ì—í¬í¬ ë™ì•ˆì˜ í•™ìŠµì„ ìˆ˜í–‰í•˜ëŠ” ë©”ì„œë“œ
        
        Args:
            criterion: ì†ì‹¤ í•¨ìˆ˜
            optimizer: ìµœì í™” ì•Œê³ ë¦¬ì¦˜
            
        Returns:
            dict: í•™ìŠµ ì†ì‹¤ê³¼ ì •í™•ë„ë¥¼ ë‹´ì€ ë”•ì…”ë„ˆë¦¬
        """
        
        running_loss = 0.0  # ëˆ„ì  ì†ì‹¤ê°’
        correct = 0         # ì˜¬ë°”ë¥´ê²Œ ì˜ˆì¸¡í•œ ìƒ˜í”Œ ìˆ˜
        total = 0           # ì „ì²´ ìƒ˜í”Œ ìˆ˜
        
        # í•™ìŠµ ë°ì´í„°ë¥¼ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
        for data, target in tqdm(self.train_loader, desc='Training'):
            
            # ========== ë°ì´í„°ë¥¼ GPUë¡œ ì´ë™ ==========
            # non_blocking=True: CPU-GPU ë°ì´í„° ì „ì†¡ì„ ë¹„ë™ê¸°ì ìœ¼ë¡œ ìˆ˜í–‰ (ì†ë„ í–¥ìƒ)
            data = data.to(self.device, non_blocking=True)
            target = target.to(self.device, non_blocking=True)

            # ========== ê¸°ìš¸ê¸° ì´ˆê¸°í™” ==========
            # ì´ì „ ë°°ì¹˜ì˜ ê¸°ìš¸ê¸°ë¥¼ ì œê±° (PyTorchëŠ” ê¸°ìš¸ê¸°ë¥¼ ëˆ„ì í•˜ë¯€ë¡œ)
            optimizer.zero_grad()
            
            # ========== ìˆœì „íŒŒ (Forward Pass) ==========
            # ëª¨ë¸ì— ì…ë ¥ ë°ì´í„°ë¥¼ í†µê³¼ì‹œì¼œ ì˜ˆì¸¡ê°’ ê³„ì‚°
            output = self.model(data)

            # ========== ì†ì‹¤ ê³„ì‚° ==========
            # ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ ì‚¬ì´ì˜ ì°¨ì´(ì†ì‹¤) ê³„ì‚°
            loss = criterion(output, target)
            
            # ========== ì—­ì „íŒŒ (Backward Pass) ==========
            # ì†ì‹¤ì— ëŒ€í•œ ê° íŒŒë¼ë¯¸í„°ì˜ ê¸°ìš¸ê¸° ê³„ì‚°
            loss.backward()
            
            # ========== íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ ==========
            # ê³„ì‚°ëœ ê¸°ìš¸ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
            optimizer.step()
            
            # ========== ë©”íŠ¸ë¦­ ê³„ì‚° ==========
            running_loss += loss.item()  # ë°°ì¹˜ ì†ì‹¤ì„ ëˆ„ì  ì†ì‹¤ì— ì¶”ê°€
            
            # ì˜ˆì¸¡ í´ë˜ìŠ¤ ê³„ì‚° (ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ í´ë˜ìŠ¤)
            _, predicted = torch.max(output.data, 1)
            
            total += target.size(0)  # ì „ì²´ ìƒ˜í”Œ ìˆ˜ ì¦ê°€
            correct += (predicted == target).sum().item()  # ì •ë‹µ ê°œìˆ˜ ì¦ê°€
        
        # í‰ê·  ì†ì‹¤ê³¼ ì •í™•ë„ ë°˜í™˜
        return {
            'loss': running_loss / len(self.train_loader),  # í‰ê·  ì†ì‹¤
            'acc': 100. * correct / total                   # ì •í™•ë„ (%)
        }
    
    def _validate_epoch(self, criterion):
        """
        í•œ ì—í¬í¬ ë™ì•ˆì˜ ê²€ì¦ì„ ìˆ˜í–‰í•˜ëŠ” ë©”ì„œë“œ
        
        Args:
            criterion: ì†ì‹¤ í•¨ìˆ˜
            
        Returns:
            dict: ê²€ì¦ ì†ì‹¤ê³¼ ì •í™•ë„ë¥¼ ë‹´ì€ ë”•ì…”ë„ˆë¦¬
        """
        
        running_loss = 0.0  # ëˆ„ì  ì†ì‹¤ê°’
        correct = 0         # ì˜¬ë°”ë¥´ê²Œ ì˜ˆì¸¡í•œ ìƒ˜í”Œ ìˆ˜
        total = 0           # ì „ì²´ ìƒ˜í”Œ ìˆ˜
        
        # ========== ê²€ì¦ ëª¨ë“œ ==========
        # torch.no_grad(): ê¸°ìš¸ê¸° ê³„ì‚°ì„ ë¹„í™œì„±í™”í•˜ì—¬ ë©”ëª¨ë¦¬ ì ˆì•½ ë° ì†ë„ í–¥ìƒ
        # ê²€ì¦ ì‹œì—ëŠ” íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ê°€ ì—†ìœ¼ë¯€ë¡œ ê¸°ìš¸ê¸° ê³„ì‚° ë¶ˆí•„ìš”
        with torch.no_grad():
            
            # ê²€ì¦ ë°ì´í„°ë¥¼ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
            for data, target in tqdm(self.val_loader, desc='Validating'):
                
                # ========== ë°ì´í„°ë¥¼ GPUë¡œ ì´ë™ ==========
                data, target = data.to(self.device), target.to(self.device)
                
                # ========== ìˆœì „íŒŒ (Forward Pass) ==========
                # ëª¨ë¸ì— ì…ë ¥ ë°ì´í„°ë¥¼ í†µê³¼ì‹œì¼œ ì˜ˆì¸¡ê°’ ê³„ì‚°
                output = self.model(data)
                
                # ========== ì†ì‹¤ ê³„ì‚° ==========
                # ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ ì‚¬ì´ì˜ ì°¨ì´(ì†ì‹¤) ê³„ì‚°
                loss = criterion(output, target)
                
                # ========== ë©”íŠ¸ë¦­ ê³„ì‚° ==========
                running_loss += loss.item()  # ë°°ì¹˜ ì†ì‹¤ì„ ëˆ„ì  ì†ì‹¤ì— ì¶”ê°€
                
                # ì˜ˆì¸¡ í´ë˜ìŠ¤ ê³„ì‚° (ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ í´ë˜ìŠ¤)
                _, predicted = torch.max(output.data, 1)
                
                total += target.size(0)  # ì „ì²´ ìƒ˜í”Œ ìˆ˜ ì¦ê°€
                correct += (predicted == target).sum().item()  # ì •ë‹µ ê°œìˆ˜ ì¦ê°€
        
        # í‰ê·  ì†ì‹¤ê³¼ ì •í™•ë„ ë°˜í™˜
        return {
            'loss': running_loss / len(self.val_loader),  # í‰ê·  ì†ì‹¤
            'acc': 100. * correct / total                 # ì •í™•ë„ (%)
        }
    
    @staticmethod
    def plot_history(history):
        """
        í•™ìŠµ ê³¼ì •ì˜ ì†ì‹¤ê³¼ ì •í™•ë„ ë³€í™”ë¥¼ ì‹œê°í™”í•˜ëŠ” ì •ì  ë©”ì„œë“œ
        
        Args:
            history (dict): í•™ìŠµ íˆìŠ¤í† ë¦¬ ë”•ì…”ë„ˆë¦¬
                - 'train_loss': ì—í¬í¬ë³„ í•™ìŠµ ì†ì‹¤ ë¦¬ìŠ¤íŠ¸
                - 'val_loss': ì—í¬í¬ë³„ ê²€ì¦ ì†ì‹¤ ë¦¬ìŠ¤íŠ¸  
                - 'train_acc': ì—í¬í¬ë³„ í•™ìŠµ ì •í™•ë„ ë¦¬ìŠ¤íŠ¸
                - 'val_acc': ì—í¬í¬ë³„ ê²€ì¦ ì •í™•ë„ ë¦¬ìŠ¤íŠ¸
        """
        
        # 2ê°œì˜ ì„œë¸Œí”Œë¡¯ì„ ê°€ì§„ ê·¸ë˜í”„ ìƒì„± (ê°€ë¡œ 12, ì„¸ë¡œ 4 ì¸ì¹˜)
        plt.figure(figsize=(12, 4))
        
        # ========== ì†ì‹¤ ê·¸ë˜í”„ (ì™¼ìª½) ==========
        plt.subplot(1, 2, 1)  # 1í–‰ 2ì—´ ì¤‘ ì²« ë²ˆì§¸
        plt.plot(history['train_loss'], label='Train Loss')  # í•™ìŠµ ì†ì‹¤ ê³¡ì„ 
        plt.plot(history['val_loss'], label='Val Loss')      # ê²€ì¦ ì†ì‹¤ ê³¡ì„ 
        plt.title('Loss History')    # ê·¸ë˜í”„ ì œëª©
        plt.xlabel('Epoch')          # xì¶• ë ˆì´ë¸”
        plt.ylabel('Loss')           # yì¶• ë ˆì´ë¸”
        plt.legend()                 # ë²”ë¡€ í‘œì‹œ
        
        # ========== ì •í™•ë„ ê·¸ë˜í”„ (ì˜¤ë¥¸ìª½) ==========
        plt.subplot(1, 2, 2)  # 1í–‰ 2ì—´ ì¤‘ ë‘ ë²ˆì§¸
        plt.plot(history['train_acc'], label='Train Acc')    # í•™ìŠµ ì •í™•ë„ ê³¡ì„ 
        plt.plot(history['val_acc'], label='Val Acc')        # ê²€ì¦ ì •í™•ë„ ê³¡ì„ 
        plt.title('Accuracy History') # ê·¸ë˜í”„ ì œëª©
        plt.xlabel('Epoch')           # xì¶• ë ˆì´ë¸”
        plt.ylabel('Accuracy (%)')    # yì¶• ë ˆì´ë¸”
        plt.legend()                  # ë²”ë¡€ í‘œì‹œ
        
        # ì„œë¸Œí”Œë¡¯ë“¤ ê°„ì˜ ê°„ê²© ìë™ ì¡°ì •
        plt.tight_layout()
        
        # ê·¸ë˜í”„ í™”ë©´ì— ì¶œë ¥
        plt.show()

# ===============================
# Inference Functions - í•™ìŠµëœ ëª¨ë¸ì„ ì‚¬ìš©í•œ ì¶”ë¡  í•¨ìˆ˜ë“¤
# ===============================
def load_trained_model(model_path, device='cuda'):
    """
    ì €ì¥ëœ ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ ë¡œë“œí•˜ì—¬ ì¶”ë¡ ìš© ëª¨ë¸ ìƒì„±
    
    Args:
        model_path (str): ì €ì¥ëœ ëª¨ë¸ íŒŒì¼ ê²½ë¡œ (.pth íŒŒì¼)
        device (str): ëª¨ë¸ì„ ë¡œë“œí•  ë””ë°”ì´ìŠ¤ ('cuda' ë˜ëŠ” 'cpu')
        
    Returns:
        torch.nn.Module: ë¡œë“œëœ ëª¨ë¸ (í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •ë¨)
    """
    
    # ë™ì¼í•œ êµ¬ì¡°ì˜ ìƒˆ ëª¨ë¸ ìƒì„±
    model = create_emotion_classifier()
    
    # ì €ì¥ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ (ë””ë°”ì´ìŠ¤ í˜¸í™˜ì„±ì„ ìœ„í•´ map_location ì‚¬ìš©)
    model.load_state_dict(torch.load(model_path, map_location=device))
    
    # ëª¨ë¸ì„ ì§€ì •ëœ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
    model.to(device)
    
    # í‰ê°€ ëª¨ë“œë¡œ ì„¤ì • (dropout, batchnorm ë¹„í™œì„±í™”)
    model.eval()
    
    return model

def predict_emotion(model, image_path, device='cuda'):
    """
    ë‹¨ì¼ ì´ë¯¸ì§€ì— ëŒ€í•œ ê°ì • ì˜ˆì¸¡ ìˆ˜í–‰
    
    Args:
        model: í•™ìŠµëœ ê°ì • ë¶„ë¥˜ ëª¨ë¸
        image_path (str): ì˜ˆì¸¡í•  ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ
        device (str): ì¶”ë¡ ì— ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤
        
    Returns:
        dict: ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë‹´ì€ ë”•ì…”ë„ˆë¦¬
            - 'emotion': ì˜ˆì¸¡ëœ ê°ì • í´ë˜ìŠ¤ëª…
            - 'confidence': ì˜ˆì¸¡ ì‹ ë¢°ë„ (0~1)
            - 'all_probabilities': ëª¨ë“  ê°ì • í´ë˜ìŠ¤ë³„ í™•ë¥ 
    """
    
    # 7ê°€ì§€ ê°ì • í´ë˜ìŠ¤ ë ˆì´ë¸” (ìˆ«ì ì¸ë±ìŠ¤ì— ëŒ€ì‘)
    emotion_labels = [
        'angry',     # 0: í™”ë‚¨
        'disgust',   # 1: í˜ì˜¤
        'fear',      # 2: ë‘ë ¤ì›€
        'happy',     # 3: í–‰ë³µ
        'neutral',   # 4: ì¤‘ë¦½
        'sad',       # 5: ìŠ¬í””
        'surprise'   # 6: ë†€ëŒ
    ]
    
    # ========== ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ==========
    # ê²€ì¦ìš© ì „ì²˜ë¦¬ ë³€í™˜ ì ìš© (ë°ì´í„° ì¦ê°• ì—†ìŒ)
    transform = get_transforms(is_train=False)
    
    # ì´ë¯¸ì§€ ë¡œë“œ ë° RGB ë³€í™˜
    image = Image.open(image_path).convert('RGB')
    
    # ì „ì²˜ë¦¬ ì ìš© í›„ ë°°ì¹˜ ì°¨ì› ì¶”ê°€ (1, 3, 224, 224)
    image_tensor = transform(image).unsqueeze(0).to(device)
    
    # ========== ì˜ˆì¸¡ ìˆ˜í–‰ ==========
    # ê¸°ìš¸ê¸° ê³„ì‚° ë¹„í™œì„±í™” (ì¶”ë¡  ì‹œì—ëŠ” ë¶ˆí•„ìš”)
    with torch.no_grad():
        
        # ëª¨ë¸ ìˆœì „íŒŒë¡œ ì˜ˆì¸¡ê°’ ê³„ì‚°
        output = model(image_tensor)
        
        # ë¡œì§“ì„ í™•ë¥ ë¡œ ë³€í™˜ (softmax ì ìš©)
        probabilities = torch.softmax(output, dim=1)
        
        # ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ í´ë˜ìŠ¤ ì¸ë±ìŠ¤ ì°¾ê¸°
        predicted_class = torch.argmax(probabilities, dim=1).item()
        
        # ì˜ˆì¸¡ëœ í´ë˜ìŠ¤ì˜ ì‹ ë¢°ë„ (í™•ë¥ ê°’)
        confidence = probabilities[0][predicted_class].item()
    
    # ========== ê²°ê³¼ ë°˜í™˜ ==========
    return {
        'emotion': emotion_labels[predicted_class],  # ì˜ˆì¸¡ëœ ê°ì •ëª…
        'confidence': confidence,                    # ì‹ ë¢°ë„
        'all_probabilities': {                       # ëª¨ë“  í´ë˜ìŠ¤ë³„ í™•ë¥ 
            emotion_labels[i]: prob.item() 
            for i, prob in enumerate(probabilities[0])
        }
    }

# ===============================
# Main Execution - í”„ë¡œê·¸ë¨ ë©”ì¸ ì‹¤í–‰ë¶€
# ===============================
def main():
    """
    FER2013 ê°ì • ì¸ì‹ ëª¨ë¸ì˜ ì „ì²´ í•™ìŠµ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜
    
    ì‹¤í–‰ ë‹¨ê³„:
    1. í™˜ê²½ ì„¤ì • ë° ë°ì´í„° í™•ì¸
    2. ë°ì´í„°ë¡œë” ìƒì„± 
    3. ëª¨ë¸ ìƒì„±
    4. 1ë‹¨ê³„ í•™ìŠµ: ë¶„ë¥˜ê¸°ë§Œ í•™ìŠµ (ë°±ë³¸ ê³ ì •)
    5. 2ë‹¨ê³„ í•™ìŠµ: ì „ì²´ ë„¤íŠ¸ì›Œí¬ ë¯¸ì„¸ì¡°ì •
    6. í•™ìŠµ ê²°ê³¼ ì‹œê°í™”
    """
    
    # ========== ì‹œì‘ ë©”ì‹œì§€ ë° í™˜ê²½ ì •ë³´ ì¶œë ¥ ==========
    print("ğŸ­ FER2013 ê°ì • ì¸ì‹ ëª¨ë¸ í•™ìŠµ ì‹œì‘")
    print(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {Config.DEVICE}")
    print(f"PyTorch ë²„ì „: {torch.__version__}")
    
    # ========== ë°ì´í„°ì…‹ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ ==========
    if not os.path.exists(Config.DATA_DIR):
        print(f"âŒ ë°ì´í„° ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {Config.DATA_DIR}")
        print("FER2013 ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œí•˜ê³  ì˜¬ë°”ë¥¸ ê²½ë¡œì— ë°°ì¹˜í•´ì£¼ì„¸ìš”.")
        print("\në°ì´í„°ì…‹ êµ¬ì¡°:")
        print("fer2013/")
        print("â”œâ”€â”€ train/")
        print("â”‚   â”œâ”€â”€ angry/")
        print("â”‚   â”œâ”€â”€ disgust/")
        print("â”‚   â”œâ”€â”€ fear/")
        print("â”‚   â”œâ”€â”€ happy/")
        print("â”‚   â”œâ”€â”€ neutral/")
        print("â”‚   â”œâ”€â”€ sad/")
        print("â”‚   â””â”€â”€ surprise/")
        print("â””â”€â”€ test/ (ê°™ì€ êµ¬ì¡°)")
        return
    
    try:
        # ========== 1. ë°ì´í„° ë¡œë” ìƒì„± ==========
        print("\nğŸ“Š ë°ì´í„° ë¡œë” ìƒì„± ì¤‘...")
        train_loader, val_loader = create_dataloaders(
            data_dir=Config.DATA_DIR,
            batch_size=Config.BATCH_SIZE,
            num_workers=Config.NUM_WORKERS
        )
        print(f"âœ… ë°ì´í„° ë¡œë” ìƒì„± ì™„ë£Œ (ë°°ì¹˜ í¬ê¸°: {Config.BATCH_SIZE})")
        
        # ========== 2. ëª¨ë¸ ìƒì„± ==========
        print("\nğŸ¤– ëª¨ë¸ ìƒì„± ì¤‘...")
        model = create_emotion_classifier(
            num_classes=Config.NUM_CLASSES,
            backbone_name=Config.MODEL_NAME
        )
        
        # ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        print(f"âœ… ëª¨ë¸ ìƒì„± ì™„ë£Œ")
        print(f"   ì „ì²´ íŒŒë¼ë¯¸í„°: {total_params:,}")
        print(f"   í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: {trainable_params:,}")
        
        # ========== 3. íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™” ==========
        print(f"\nğŸ”§ íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™” ì¤‘... (ë””ë°”ì´ìŠ¤: {Config.DEVICE})")
        trainer = EmotionTrainer(
            model=model,
            train_loader=train_loader,
            val_loader=val_loader,
            device=Config.DEVICE
        )
        print("âœ… íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™” ì™„ë£Œ")
        
        # ========== 4. 1ë‹¨ê³„ í•™ìŠµ: ë¶„ë¥˜ê¸°ë§Œ í•™ìŠµ ==========
        print(f"\nğŸ¯ 1ë‹¨ê³„ í•™ìŠµ ì‹œì‘: ë¶„ë¥˜ê¸° ë ˆì´ì–´ë§Œ í•™ìŠµ ({Config.CLASSIFIER_EPOCHS} ì—í¬í¬)")
        print("   - ë°±ë³¸ ë„¤íŠ¸ì›Œí¬: ê³ ì • (ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ ì•ˆí•¨)")
        print("   - ë¶„ë¥˜ê¸° ë ˆì´ì–´: í•™ìŠµ (ìƒˆë¡œìš´ ì‘ì—…ì— ë§ê²Œ ì¡°ì •)")
        
        history1 = trainer.train_classifier_only(
            epochs=Config.CLASSIFIER_EPOCHS,
            lr=Config.CLASSIFIER_LR
        )
        print("âœ… 1ë‹¨ê³„ í•™ìŠµ ì™„ë£Œ!")
        
        # ========== 5. 2ë‹¨ê³„ í•™ìŠµ: ì „ì²´ ë„¤íŠ¸ì›Œí¬ ë¯¸ì„¸ì¡°ì • ==========
        print(f"\nğŸ”¬ 2ë‹¨ê³„ í•™ìŠµ ì‹œì‘: ì „ì²´ ë„¤íŠ¸ì›Œí¬ ë¯¸ì„¸ì¡°ì • ({Config.FULL_NETWORK_EPOCHS} ì—í¬í¬)")
        print(f"   - ë°±ë³¸ ë„¤íŠ¸ì›Œí¬: ë¯¸ì„¸ì¡°ì • (í•™ìŠµë¥ : {Config.BACKBONE_LR})")
        print(f"   - ë¶„ë¥˜ê¸° ë ˆì´ì–´: ê³„ì† í•™ìŠµ (í•™ìŠµë¥ : {Config.CLASSIFIER_LR_FINETUNE})")
        
        history2 = trainer.train_full_network(
            epochs=Config.FULL_NETWORK_EPOCHS,
            backbone_lr=Config.BACKBONE_LR,
            classifier_lr=Config.CLASSIFIER_LR_FINETUNE
        )
        print("âœ… 2ë‹¨ê³„ í•™ìŠµ ì™„ë£Œ!")
        
        # ========== 6. í•™ìŠµ ê²°ê³¼ ì‹œê°í™” ==========
        print("\nğŸ“ˆ í•™ìŠµ ê²°ê³¼ ì‹œê°í™”...")
        print("1ë‹¨ê³„ í•™ìŠµ ê²°ê³¼:")
        EmotionTrainer.plot_history(history1)
        
        print("2ë‹¨ê³„ í•™ìŠµ ê²°ê³¼:")
        EmotionTrainer.plot_history(history2)
        
        # ========== í•™ìŠµ ì™„ë£Œ ë©”ì‹œì§€ ==========
        print("\nğŸ‰ ëª¨ë“  í•™ìŠµì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print("\nğŸ“ ì €ì¥ëœ íŒŒì¼:")
        print("  - best_model.pth: ìµœê³  ì„±ëŠ¥ì„ ë‹¬ì„±í•œ ëª¨ë¸ ê°€ì¤‘ì¹˜")
        print("\nğŸ’¡ ëª¨ë¸ ì‚¬ìš© ì˜ˆì‹œ:")
        print("  model = load_trained_model('best_model.pth')")
        print("  result = predict_emotion(model, 'image.jpg')")
        
    except Exception as e:
        print(f"\nâŒ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        print("\nğŸ” ìƒì„¸ ì˜¤ë¥˜ ì •ë³´:")
        import traceback
        traceback.print_exc()
        print("\nğŸ’¡ í•´ê²° ë°©ë²•:")
        print("  1. ë°ì´í„°ì…‹ ê²½ë¡œê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸")
        print("  2. GPU ë©”ëª¨ë¦¬ê°€ ì¶©ë¶„í•œì§€ í™•ì¸ (ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°)")
        print("  3. í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ëª¨ë‘ ì„¤ì¹˜ë˜ì—ˆëŠ”ì§€ í™•ì¸")

if __name__ == "__main__":
    main()
