{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cc1bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: 라이브러리 설치 및 import\n",
    "# 필요한 라이브러리 설치\n",
    "%pip install fastapi uvicorn python-multipart jinja2 aiofiles\n",
    "%pip install torch torchvision transformers ultralytics opencv-python pillow numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda04f4f",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98bca5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\orix4\\anaconda3\\envs\\opencv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 라이브러리 import 완료\n"
     ]
    }
   ],
   "source": [
    "# 기본 라이브러리 import\n",
    "from fastapi import FastAPI, Request, File, UploadFile, HTTPException\n",
    "from fastapi.responses import HTMLResponse, JSONResponse\n",
    "from fastapi.staticfiles import StaticFiles\n",
    "from fastapi.templating import Jinja2Templates\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "import uvicorn\n",
    "import os\n",
    "import threading\n",
    "import asyncio\n",
    "import io\n",
    "import base64\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "from threading import Lock\n",
    "from fastapi import WebSocket, WebSocketDisconnect\n",
    "import asyncio\n",
    "\n",
    "\n",
    "# AI 모델 라이브러리\n",
    "import cv2\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification, ViTImageProcessor, ViTForImageClassification, pipeline\n",
    "from ultralytics import YOLO\n",
    "\n",
    "print(\"✅ 라이브러리 import 완료\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b381822",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c6d4785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 감정 예측 함수 정의 완료\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: 모델 초기화 및 설정\n",
    "# 글로벌 변수로 모델들 저장\n",
    "# 전역 변수에 실시간 처리용 변수들 추가\n",
    "emotion_model = None\n",
    "yolo_model = None\n",
    "processor = None\n",
    "device = None\n",
    "emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise']\n",
    "\n",
    "# 감정 분석 결과 저장용 전역 변수\n",
    "all_emotions = []\n",
    "emotion_statistics = {\n",
    "    'Angry': 0, 'Disgust': 0, 'Fear': 0, 'Happy': 0,\n",
    "    'Neutral': 0, 'Sad': 0, 'Surprise': 0\n",
    "}\n",
    "analysis_history = []\n",
    "\n",
    "# 실시간 처리용 전역 변수\n",
    "is_processing = False  # 실시간 처리 상태\n",
    "processing_thread = None  # 처리 스레드\n",
    "frame_count = 0  # 프레임 카운터\n",
    "data_lock = Lock()  # 데이터 동기화용 락\n",
    "webcam_capture = None  # 웹캠 캡처 객체\n",
    "\n",
    "\n",
    "# 통합된 감정 예측 함수 (1:1 패딩 + 224x224)\n",
    "def predict_emotion(face_image):\n",
    "    \"\"\"\n",
    "    얼굴 이미지에서 감정을 예측하는 함수\n",
    "    Args:\n",
    "        face_image: 입력 얼굴 이미지 (numpy array, RGB)\n",
    "    Returns:\n",
    "        [{'label': str, 'score': float}, ...] 상위 2개 감정 예측 리스트 (가중치 적용 용도도)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. 이미지를 1:1 비율로 정사각형 패딩\n",
    "        h, w = face_image.shape[:2]\n",
    "        max_size = max(h, w)\n",
    "        \n",
    "        # 정사각형 패딩\n",
    "        square_img = np.zeros((max_size, max_size, 3), dtype=np.uint8)\n",
    "        y_offset = (max_size - h) // 2\n",
    "        x_offset = (max_size - w) // 2\n",
    "        square_img[y_offset:y_offset+h, x_offset:x_offset+w] = face_image\n",
    "        \n",
    "        # 2. PIL 이미지로 변환\n",
    "        pil_image = Image.fromarray(square_img)\n",
    "        if pil_image.mode != 'RGB':\n",
    "            pil_image = pil_image.convert('RGB')\n",
    "        \n",
    "        # 3. ViT 전처리 (자동으로 224x224로 리사이즈됨)\n",
    "        inputs = processor(pil_image, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        # 4. 추론\n",
    "        with torch.no_grad():\n",
    "            outputs = emotion_model(**inputs)\n",
    "            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        \n",
    "        # 5. 상위 2개 결과만 반환 (가중치 적용을 위해)\n",
    "        top2_idx = torch.topk(predictions, 2).indices[0]\n",
    "        top2_scores = torch.topk(predictions, 2).values[0]\n",
    "        \n",
    "        results = []\n",
    "        for idx, score in zip(top2_idx, top2_scores):\n",
    "            results.append({\n",
    "                'label': emotion_labels[idx.item()],\n",
    "                'score': float(score.item())\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"감정 예측 오류: {e}\")\n",
    "        return [{'label': 'Error', 'score': 0.0}]\n",
    "\n",
    "print(\"✅ 감정 예측 함수 정의 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78232433",
   "metadata": {},
   "source": [
    "# 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4eab4067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 모델 로드 함수 정의 완료\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: 모델 로드 함수\n",
    "VIT_MODEL_PATH = \"./ViT_model.pth\"\n",
    "YOLO_MODEL_PATH = \"./yolov12n-face.pt\" \n",
    "\n",
    "async def load_models():\n",
    "    \"\"\"모델들을 비동기적으로 로드하는 함수\"\"\"\n",
    "    global emotion_model, yolo_model, processor, device\n",
    "    \n",
    "    try:\n",
    "        print(\"🔄 모델 로딩 중...\")\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        \n",
    "        # ViT 모델과 프로세서 로드\n",
    "        processor = ViTImageProcessor.from_pretrained(\"mo-thecreator/vit-Facial-Expression-Recognition\")\n",
    "        emotion_model = ViTForImageClassification.from_pretrained(\"mo-thecreator/vit-Facial-Expression-Recognition\")\n",
    "        checkpoint = torch.load(VIT_MODEL_PATH, map_location=torch.device(device))\n",
    "        emotion_model.load_state_dict(checkpoint)\n",
    "        emotion_model.to(device)\n",
    "        emotion_model.eval()\n",
    "        \n",
    "        # YOLO 얼굴 탐지 모델 로드\n",
    "        yolo_model = YOLO(YOLO_MODEL_PATH)\n",
    "        \n",
    "        print(\"✅ 모델 로딩 완료!\")\n",
    "        print(f\"사용 장치: {device}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 모델 로딩 실패: {e}\")\n",
    "        print(\"기본 모델로 대체합니다.\")\n",
    "        try:\n",
    "            global pipe\n",
    "            pipe = pipeline(\"image-classification\", model=\"j-hartmann/emotion-english-distilroberta-base\")\n",
    "            print(\"✅ 대체 모델 로드 완료\")\n",
    "        except:\n",
    "            print(\"❌ 대체 모델도 실패\")\n",
    "\n",
    "print(\"✅ 모델 로드 함수 정의 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2d2728",
   "metadata": {},
   "source": [
    "# 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0d34461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ FastAPI 앱 설정 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\orix4\\AppData\\Local\\Temp\\ipykernel_29144\\3086945966.py:15: DeprecationWarning: \n",
      "        on_event is deprecated, use lifespan event handlers instead.\n",
      "\n",
      "        Read more about it in the\n",
      "        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).\n",
      "        \n",
      "  @app.on_event(\"startup\")\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: FastAPI 앱 설정\n",
    "app = FastAPI(title=\"웹캠 감정 분석 API\", version=\"1.0.0\")\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "os.makedirs(\"templates\", exist_ok=True)\n",
    "templates = Jinja2Templates(directory=\"templates\")\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "async def startup_event():\n",
    "    await load_models()\n",
    "\n",
    "print(\"✅ FastAPI 앱 설정 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b16cb58",
   "metadata": {},
   "source": [
    "# 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f18062c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ AI 분석 함수 정의 완료\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: AI 분석 함수 (YOLO 필수)\n",
    "def analyze_webcam_image(image_bytes):\n",
    "    \"\"\"웹캠 이미지를 분석하여 감정을 반환하는 함수 (가중치 적용 및 데이터 저장)\"\"\"\n",
    "    global all_emotions, emotion_statistics, analysis_history\n",
    "    \n",
    "    try:\n",
    "        # 바이트 데이터를 numpy 배열로 변환\n",
    "        nparr = np.frombuffer(image_bytes, np.uint8)\n",
    "        frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
    "        \n",
    "        if frame is None:\n",
    "            return \"이미지를 읽을 수 없습니다.\"\n",
    "        \n",
    "        # RGB로 변환\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        analysis_results = []\n",
    "        current_analysis_emotions = []  # 현재 분석의 감정들 (가중치 적용)\n",
    "        \n",
    "        # YOLO로 얼굴 탐지\n",
    "        try:\n",
    "            results = yolo_model(frame, verbose=False)\n",
    "            face_detected = False\n",
    "            \n",
    "            for result in results:\n",
    "                boxes = result.boxes\n",
    "                if boxes is not None and len(boxes) > 0:\n",
    "                    for box in boxes:\n",
    "                        # 신뢰도 0.8 이상만 처리 (비디오 코드와 동일)\n",
    "                        if box.cls[0] == 0 and box.conf[0] > 0.8:\n",
    "                            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                            confidence = float(box.conf[0])\n",
    "                            \n",
    "                            # 얼굴 영역 추정 (상반신의 상위 1/3 정도)\n",
    "                            face_height = int((y2 - y1) * 0.3)\n",
    "                            face_y1 = y1\n",
    "                            face_y2 = y1 + face_height\n",
    "                            face_x1 = x1 + int((x2 - x1) * 0.2)\n",
    "                            face_x2 = x2 - int((x2 - x1) * 0.2)\n",
    "                            \n",
    "                            # 유효한 영역인지 확인\n",
    "                            if (face_y2 > face_y1 and face_x2 > face_x1 and \n",
    "                                face_y1 >= 0 and face_x1 >= 0 and \n",
    "                                face_y2 < frame.shape[0] and face_x2 < frame.shape[1]):\n",
    "                                \n",
    "                                face_crop = frame_rgb[face_y1:face_y2, face_x1:face_x2]\n",
    "                                \n",
    "                                if face_crop.size > 0:\n",
    "                                    # 감정 예측\n",
    "                                    emotion_results = predict_emotion(face_crop)\n",
    "                                    face_detected = True\n",
    "                                    \n",
    "                                    # 상위 2개 감정에 가중치 적용 (비디오 코드와 동일)\n",
    "                                    if len(emotion_results) >= 2:\n",
    "                                        first_emotion = emotion_results[0]['label']\n",
    "                                        second_emotion = emotion_results[1]['label']\n",
    "                                        first_score = emotion_results[0]['score']\n",
    "                                        second_score = emotion_results[1]['score']\n",
    "                                        \n",
    "                                        # 신뢰도 차이가 클수록 첫 번째에 더 큰 가중치\n",
    "                                        score_ratio = first_score / (second_score + 0.01)\n",
    "                                        if score_ratio > 2.0:  # 2배 이상 차이나면\n",
    "                                            current_analysis_emotions.extend([first_emotion] * 3)  # 3배 가중치\n",
    "                                            current_analysis_emotions.extend([second_emotion])\n",
    "                                        else:\n",
    "                                            current_analysis_emotions.extend([first_emotion])\n",
    "                                            current_analysis_emotions.extend([second_emotion])\n",
    "                                    \n",
    "                                    analysis_results.append(f\"👤 사람 탐지됨 (신뢰도: {confidence:.2f})\")\n",
    "                                    analysis_results.append(f\"📊 감정 분석 결과:\")\n",
    "                                    \n",
    "                                    for i, emotion in enumerate(emotion_results[:3]):\n",
    "                                        emoji_map = {\n",
    "                                            'Angry': '😡', 'Disgust': '🤢', 'Fear': '😨',\n",
    "                                            'Happy': '😄', 'Neutral': '😐', 'Sad': '😭', 'Surprise': '😮'\n",
    "                                        }\n",
    "                                        emoji = emoji_map.get(emotion['label'], '❓')\n",
    "                                        analysis_results.append(\n",
    "                                            f\"  {i+1}. {emoji} {emotion['label']}: {emotion['score']:.1%}\"\n",
    "                                        )\n",
    "                                    break\n",
    "            \n",
    "            # 감정 데이터 저장 (얼굴이 감지된 경우만)\n",
    "            if face_detected and current_analysis_emotions:\n",
    "                # 전체 감정 리스트에 추가\n",
    "                all_emotions.extend(current_analysis_emotions)\n",
    "                \n",
    "                # 감정별 카운트 업데이트\n",
    "                for emotion in current_analysis_emotions:\n",
    "                    if emotion in emotion_statistics:\n",
    "                        emotion_statistics[emotion] += 1\n",
    "                \n",
    "                # 분석 히스토리에 추가\n",
    "                analysis_history.append({\n",
    "                    'timestamp': datetime.datetime.now().isoformat(),\n",
    "                    'emotions': current_analysis_emotions.copy(),\n",
    "                    'confidence': confidence\n",
    "                })\n",
    "                \n",
    "                # 가중치 적용된 감정 정보 추가\n",
    "                analysis_results.append(f\"\\n🔢 가중치 적용된 감정: {', '.join(current_analysis_emotions)}\")\n",
    "            \n",
    "            if not face_detected:\n",
    "                analysis_results.append(\"👤 사람이 감지되지 않았습니다.\")\n",
    "                analysis_results.append(\"💡 카메라에 얼굴이 잘 보이도록 조정해주세요.\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            analysis_results.append(f\"❌ 얼굴 탐지 오류: {str(e)}\")\n",
    "        \n",
    "        # 분석 시간 추가\n",
    "        analysis_results.append(f\"\\n⏰ 분석 시간: {datetime.datetime.now().strftime('%H:%M:%S')}\")\n",
    "        \n",
    "        # 누적 통계 정보 추가\n",
    "        if all_emotions:\n",
    "            analysis_results.append(f\"\\n📊 누적 분석 통계:\")\n",
    "            for emotion, count in emotion_statistics.items():\n",
    "                if count > 0:\n",
    "                    emoji_map = {\n",
    "                        'Angry': '😡', 'Disgust': '🤢', 'Fear': '😨',\n",
    "                        'Happy': '😄', 'Neutral': '😐', 'Sad': '😭', 'Surprise': '😮'\n",
    "                    }\n",
    "                    emoji = emoji_map.get(emotion, '❓')\n",
    "                    analysis_results.append(f\"  {emoji} {emotion}: {count}회\")\n",
    "        \n",
    "        return \"\\n\".join(analysis_results)\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"❌ 이미지 분석 중 오류 발생: {str(e)}\"\n",
    "\n",
    "print(\"✅ AI 분석 함수 정의 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88373194",
   "metadata": {},
   "source": [
    "# 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847c2c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ FastAPI 라우트가 설정되었습니다.\n",
      "🚀 AI 감정 분석 서버가 시작되었습니다!\n",
      "📱 웹페이지 접속: http://127.0.0.1:8000\n",
      "📊 모델 상태 확인: http://127.0.0.1:8000/model-status\n",
      "📚 API 문서: http://127.0.0.1:8000/docs\n",
      "\n",
      "🤖 지원 기능:\n",
      "- 실시간 웹캠 캡처\n",
      "- YOLO 기반 얼굴/사람 탐지 (필수)\n",
      "- ViT 기반 7가지 감정 분석 (1:1 패딩 + 224x224)\n",
      "- 실시간 결과 표시\n",
      "\n",
      "⚠️  주의: 첫 실행 시 모델 다운로드로 시간이 소요될 수 있습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [29144]\n",
      "INFO:     Waiting for application startup.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 모델 로딩 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 모델 로딩 완료!\n",
      "사용 장치: cpu\n",
      "INFO:     127.0.0.1:7610 - \"GET / HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:  Unsupported upgrade request.\n",
      "WARNING:  No supported WebSocket library detected. Please use \"pip install 'uvicorn[standard]'\", or install 'websockets' or 'wsproto' manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:4684 - \"GET /ws HTTP/1.1\" 404 Not Found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:  Unsupported upgrade request.\n",
      "WARNING:  No supported WebSocket library detected. Please use \"pip install 'uvicorn[standard]'\", or install 'websockets' or 'wsproto' manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:3629 - \"GET /ws HTTP/1.1\" 404 Not Found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:  Unsupported upgrade request.\n",
      "WARNING:  No supported WebSocket library detected. Please use \"pip install 'uvicorn[standard]'\", or install 'websockets' or 'wsproto' manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:9305 - \"GET /ws HTTP/1.1\" 404 Not Found\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: FastAPI 라우트 정의 \n",
    "@app.get(\"/\", response_class=HTMLResponse)\n",
    "async def main_page(request: Request):\n",
    "    return templates.TemplateResponse(\"index.html\", {\"request\": request})\n",
    "\n",
    "@app.post(\"/analyze\")\n",
    "async def analyze_image(file: UploadFile = File(...)):\n",
    "    try:\n",
    "        if not file.content_type.startswith(\"image/\"):\n",
    "            raise HTTPException(status_code=400, detail=\"이미지 파일만 업로드 가능합니다.\")\n",
    "        \n",
    "        image_data = await file.read()\n",
    "        print(f\"[DEBUG] 이미지 바이트 크기: {len(image_data)}\")\n",
    "        analysis_result = analyze_webcam_image(image_data)\n",
    "        print(f\"[DEBUG] 분석 결과:\\n{analysis_result}\")\n",
    "        \n",
    "        return JSONResponse(content={\n",
    "            \"status\": \"success\",\n",
    "            \"analysis\": analysis_result,\n",
    "            \"image_size\": len(image_data)\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] 분석 중 오류: {e}\")\n",
    "        return JSONResponse(content={\n",
    "            \"status\": \"error\",\n",
    "            \"analysis\": f\"분석 중 오류가 발생했습니다: {str(e)}\"\n",
    "        }, status_code=500)\n",
    "\n",
    "@app.get(\"/model-status\")\n",
    "async def model_status():\n",
    "    global emotion_model, yolo_model\n",
    "    status = {\n",
    "        \"emotion_model\": emotion_model is not None,\n",
    "        \"yolo_model\": yolo_model is not None,\n",
    "        \"device\": device\n",
    "    }\n",
    "    return JSONResponse(content=status)\n",
    "\n",
    "@app.get(\"/emotion-statistics\")\n",
    "async def get_emotion_statistics():\n",
    "    \"\"\"감정 분석 통계 조회\"\"\"\n",
    "    global all_emotions, emotion_statistics, analysis_history\n",
    "    \n",
    "    return JSONResponse(content={\n",
    "        \"total_analyses\": len(analysis_history),\n",
    "        \"total_emotions\": len(all_emotions),\n",
    "        \"emotion_counts\": emotion_statistics,\n",
    "        \"recent_analyses\": analysis_history[-10:] if len(analysis_history) > 10 else analysis_history\n",
    "    })\n",
    "\n",
    "@app.post(\"/reset-statistics\")\n",
    "async def reset_statistics():\n",
    "    \"\"\"감정 분석 통계 초기화\"\"\"\n",
    "    global all_emotions, emotion_statistics, analysis_history\n",
    "    \n",
    "    all_emotions.clear()\n",
    "    analysis_history.clear()\n",
    "    for emotion in emotion_statistics:\n",
    "        emotion_statistics[emotion] = 0\n",
    "    \n",
    "    return JSONResponse(content={\"status\": \"success\", \"message\": \"통계가 초기화되었습니다.\"})\n",
    "\n",
    "@app.get(\"/export-emotions\")\n",
    "async def export_emotions():\n",
    "    \"\"\"감정 데이터 내보내기 (데이터베이스 저장 전 확인용)\"\"\"\n",
    "    global all_emotions, emotion_statistics, analysis_history\n",
    "    \n",
    "    return JSONResponse(content={\n",
    "        \"all_emotions\": all_emotions,\n",
    "        \"statistics\": emotion_statistics,\n",
    "        \"history\": analysis_history,\n",
    "        \"export_time\": datetime.datetime.now().isoformat()\n",
    "    })\n",
    "\n",
    "@app.websocket(\"/ws\")\n",
    "async def websocket_endpoint(websocket: WebSocket):\n",
    "    await websocket.accept()\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            # JSON 형태로 받기\n",
    "            data = await websocket.receive_json()\n",
    "            \n",
    "            # base64 이미지 디코딩\n",
    "            if 'image' in data:\n",
    "                image_b64 = data['image']\n",
    "                image_bytes = base64.b64decode(image_b64)\n",
    "                \n",
    "                # 기존 분석 함수 사용\n",
    "                result = analyze_webcam_image(image_bytes)\n",
    "                \n",
    "                # 결과 전송\n",
    "                await websocket.send_json({\"analysis\": result})\n",
    "            \n",
    "    except WebSocketDisconnect:\n",
    "        pass\n",
    "    except Exception as e:\n",
    "        print(f\"WebSocket 오류: {e}\")\n",
    "        await websocket.send_json({\"analysis\": f\"처리 중 오류: {str(e)}\"})\n",
    "\n",
    "print(\"✅ FastAPI 라우트가 설정되었습니다.\")\n",
    "\n",
    "# 서버 실행 함수\n",
    "def run_server():\n",
    "    uvicorn.run(app, host=\"127.0.0.1\", port=8000, log_level=\"info\")\n",
    "\n",
    "# 백그라운드에서 서버 실행\n",
    "server_thread = threading.Thread(target=run_server, daemon=True)\n",
    "server_thread.start()\n",
    "\n",
    "print(\"🚀 AI 감정 분석 서버가 시작되었습니다!\")\n",
    "print(\"📱 웹페이지 접속: http://127.0.0.1:8000\")\n",
    "print(\"📊 모델 상태 확인: http://127.0.0.1:8000/model-status\")\n",
    "print(\"📚 API 문서: http://127.0.0.1:8000/docs\")\n",
    "print(\"\\n🤖 지원 기능:\")\n",
    "print(\"- 실시간 웹캠 캡처\")\n",
    "print(\"- YOLO 기반 얼굴/사람 탐지 (필수)\")\n",
    "print(\"- ViT 기반 7가지 감정 분석 (1:1 패딩 + 224x224)\")\n",
    "print(\"- 실시간 결과 표시\")\n",
    "print(\"\\n⚠️  주의: 첫 실행 시 모델 다운로드로 시간이 소요될 수 있습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc13fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import signal\n",
    "\n",
    "# 현재 프로세스의 PID 얻기\n",
    "current_pid = os.getpid()\n",
    "print(f\"현재 프로세스 PID: {current_pid}\")\n",
    "\n",
    "# 자신을 종료\n",
    "os.kill(current_pid, signal.SIGKILL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5681c7bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opencv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
