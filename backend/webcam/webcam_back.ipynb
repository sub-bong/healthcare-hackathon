{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cc1bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë° import\n",
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
    "%pip install fastapi uvicorn python-multipart jinja2 aiofiles\n",
    "%pip install torch torchvision transformers ultralytics opencv-python pillow numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda04f4f",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98bca5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\orix4\\anaconda3\\envs\\opencv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ import ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ import\n",
    "from fastapi import FastAPI, Request, File, UploadFile, HTTPException\n",
    "from fastapi.responses import HTMLResponse, JSONResponse\n",
    "from fastapi.staticfiles import StaticFiles\n",
    "from fastapi.templating import Jinja2Templates\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "import uvicorn\n",
    "import os\n",
    "import threading\n",
    "import asyncio\n",
    "import io\n",
    "import base64\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "from threading import Lock\n",
    "from fastapi import WebSocket, WebSocketDisconnect\n",
    "import asyncio\n",
    "\n",
    "\n",
    "# AI ëª¨ë¸ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "import cv2\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification, ViTImageProcessor, ViTForImageClassification, pipeline\n",
    "from ultralytics import YOLO\n",
    "\n",
    "print(\"âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ import ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b381822",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c6d4785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ê°ì • ì˜ˆì¸¡ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: ëª¨ë¸ ì´ˆê¸°í™” ë° ì„¤ì •\n",
    "# ê¸€ë¡œë²Œ ë³€ìˆ˜ë¡œ ëª¨ë¸ë“¤ ì €ì¥\n",
    "# ì „ì—­ ë³€ìˆ˜ì— ì‹¤ì‹œê°„ ì²˜ë¦¬ìš© ë³€ìˆ˜ë“¤ ì¶”ê°€\n",
    "emotion_model = None\n",
    "yolo_model = None\n",
    "processor = None\n",
    "device = None\n",
    "emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise']\n",
    "\n",
    "# ê°ì • ë¶„ì„ ê²°ê³¼ ì €ì¥ìš© ì „ì—­ ë³€ìˆ˜\n",
    "all_emotions = []\n",
    "emotion_statistics = {\n",
    "    'Angry': 0, 'Disgust': 0, 'Fear': 0, 'Happy': 0,\n",
    "    'Neutral': 0, 'Sad': 0, 'Surprise': 0\n",
    "}\n",
    "analysis_history = []\n",
    "\n",
    "# ì‹¤ì‹œê°„ ì²˜ë¦¬ìš© ì „ì—­ ë³€ìˆ˜\n",
    "is_processing = False  # ì‹¤ì‹œê°„ ì²˜ë¦¬ ìƒíƒœ\n",
    "processing_thread = None  # ì²˜ë¦¬ ìŠ¤ë ˆë“œ\n",
    "frame_count = 0  # í”„ë ˆì„ ì¹´ìš´í„°\n",
    "data_lock = Lock()  # ë°ì´í„° ë™ê¸°í™”ìš© ë½\n",
    "webcam_capture = None  # ì›¹ìº  ìº¡ì²˜ ê°ì²´\n",
    "\n",
    "\n",
    "# í†µí•©ëœ ê°ì • ì˜ˆì¸¡ í•¨ìˆ˜ (1:1 íŒ¨ë”© + 224x224)\n",
    "def predict_emotion(face_image):\n",
    "    \"\"\"\n",
    "    ì–¼êµ´ ì´ë¯¸ì§€ì—ì„œ ê°ì •ì„ ì˜ˆì¸¡í•˜ëŠ” í•¨ìˆ˜\n",
    "    Args:\n",
    "        face_image: ì…ë ¥ ì–¼êµ´ ì´ë¯¸ì§€ (numpy array, RGB)\n",
    "    Returns:\n",
    "        [{'label': str, 'score': float}, ...] ìƒìœ„ 2ê°œ ê°ì • ì˜ˆì¸¡ ë¦¬ìŠ¤íŠ¸ (ê°€ì¤‘ì¹˜ ì ìš© ìš©ë„ë„)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. ì´ë¯¸ì§€ë¥¼ 1:1 ë¹„ìœ¨ë¡œ ì •ì‚¬ê°í˜• íŒ¨ë”©\n",
    "        h, w = face_image.shape[:2]\n",
    "        max_size = max(h, w)\n",
    "        \n",
    "        # ì •ì‚¬ê°í˜• íŒ¨ë”©\n",
    "        square_img = np.zeros((max_size, max_size, 3), dtype=np.uint8)\n",
    "        y_offset = (max_size - h) // 2\n",
    "        x_offset = (max_size - w) // 2\n",
    "        square_img[y_offset:y_offset+h, x_offset:x_offset+w] = face_image\n",
    "        \n",
    "        # 2. PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜\n",
    "        pil_image = Image.fromarray(square_img)\n",
    "        if pil_image.mode != 'RGB':\n",
    "            pil_image = pil_image.convert('RGB')\n",
    "        \n",
    "        # 3. ViT ì „ì²˜ë¦¬ (ìë™ìœ¼ë¡œ 224x224ë¡œ ë¦¬ì‚¬ì´ì¦ˆë¨)\n",
    "        inputs = processor(pil_image, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        # 4. ì¶”ë¡ \n",
    "        with torch.no_grad():\n",
    "            outputs = emotion_model(**inputs)\n",
    "            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        \n",
    "        # 5. ìƒìœ„ 2ê°œ ê²°ê³¼ë§Œ ë°˜í™˜ (ê°€ì¤‘ì¹˜ ì ìš©ì„ ìœ„í•´)\n",
    "        top2_idx = torch.topk(predictions, 2).indices[0]\n",
    "        top2_scores = torch.topk(predictions, 2).values[0]\n",
    "        \n",
    "        results = []\n",
    "        for idx, score in zip(top2_idx, top2_scores):\n",
    "            results.append({\n",
    "                'label': emotion_labels[idx.item()],\n",
    "                'score': float(score.item())\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ê°ì • ì˜ˆì¸¡ ì˜¤ë¥˜: {e}\")\n",
    "        return [{'label': 'Error', 'score': 0.0}]\n",
    "\n",
    "print(\"âœ… ê°ì • ì˜ˆì¸¡ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78232433",
   "metadata": {},
   "source": [
    "# 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4eab4067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜\n",
    "VIT_MODEL_PATH = \"./ViT_model.pth\"\n",
    "YOLO_MODEL_PATH = \"./yolov12n-face.pt\" \n",
    "\n",
    "async def load_models():\n",
    "    \"\"\"ëª¨ë¸ë“¤ì„ ë¹„ë™ê¸°ì ìœ¼ë¡œ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜\"\"\"\n",
    "    global emotion_model, yolo_model, processor, device\n",
    "    \n",
    "    try:\n",
    "        print(\"ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘...\")\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        \n",
    "        # ViT ëª¨ë¸ê³¼ í”„ë¡œì„¸ì„œ ë¡œë“œ\n",
    "        processor = ViTImageProcessor.from_pretrained(\"mo-thecreator/vit-Facial-Expression-Recognition\")\n",
    "        emotion_model = ViTForImageClassification.from_pretrained(\"mo-thecreator/vit-Facial-Expression-Recognition\")\n",
    "        checkpoint = torch.load(VIT_MODEL_PATH, map_location=torch.device(device))\n",
    "        emotion_model.load_state_dict(checkpoint)\n",
    "        emotion_model.to(device)\n",
    "        emotion_model.eval()\n",
    "        \n",
    "        # YOLO ì–¼êµ´ íƒì§€ ëª¨ë¸ ë¡œë“œ\n",
    "        yolo_model = YOLO(YOLO_MODEL_PATH)\n",
    "        \n",
    "        print(\"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!\")\n",
    "        print(f\"ì‚¬ìš© ì¥ì¹˜: {device}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}\")\n",
    "        print(\"ê¸°ë³¸ ëª¨ë¸ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.\")\n",
    "        try:\n",
    "            global pipe\n",
    "            pipe = pipeline(\"image-classification\", model=\"j-hartmann/emotion-english-distilroberta-base\")\n",
    "            print(\"âœ… ëŒ€ì²´ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\")\n",
    "        except:\n",
    "            print(\"âŒ ëŒ€ì²´ ëª¨ë¸ë„ ì‹¤íŒ¨\")\n",
    "\n",
    "print(\"âœ… ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2d2728",
   "metadata": {},
   "source": [
    "# 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0d34461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… FastAPI ì•± ì„¤ì • ì™„ë£Œ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\orix4\\AppData\\Local\\Temp\\ipykernel_29144\\3086945966.py:15: DeprecationWarning: \n",
      "        on_event is deprecated, use lifespan event handlers instead.\n",
      "\n",
      "        Read more about it in the\n",
      "        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).\n",
      "        \n",
      "  @app.on_event(\"startup\")\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: FastAPI ì•± ì„¤ì •\n",
    "app = FastAPI(title=\"ì›¹ìº  ê°ì • ë¶„ì„ API\", version=\"1.0.0\")\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "os.makedirs(\"templates\", exist_ok=True)\n",
    "templates = Jinja2Templates(directory=\"templates\")\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "async def startup_event():\n",
    "    await load_models()\n",
    "\n",
    "print(\"âœ… FastAPI ì•± ì„¤ì • ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b16cb58",
   "metadata": {},
   "source": [
    "# 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f18062c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… AI ë¶„ì„ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: AI ë¶„ì„ í•¨ìˆ˜ (YOLO í•„ìˆ˜)\n",
    "def analyze_webcam_image(image_bytes):\n",
    "    \"\"\"ì›¹ìº  ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•˜ì—¬ ê°ì •ì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ (ê°€ì¤‘ì¹˜ ì ìš© ë° ë°ì´í„° ì €ì¥)\"\"\"\n",
    "    global all_emotions, emotion_statistics, analysis_history\n",
    "    \n",
    "    try:\n",
    "        # ë°”ì´íŠ¸ ë°ì´í„°ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜\n",
    "        nparr = np.frombuffer(image_bytes, np.uint8)\n",
    "        frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
    "        \n",
    "        if frame is None:\n",
    "            return \"ì´ë¯¸ì§€ë¥¼ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"\n",
    "        \n",
    "        # RGBë¡œ ë³€í™˜\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        analysis_results = []\n",
    "        current_analysis_emotions = []  # í˜„ì¬ ë¶„ì„ì˜ ê°ì •ë“¤ (ê°€ì¤‘ì¹˜ ì ìš©)\n",
    "        \n",
    "        # YOLOë¡œ ì–¼êµ´ íƒì§€\n",
    "        try:\n",
    "            results = yolo_model(frame, verbose=False)\n",
    "            face_detected = False\n",
    "            \n",
    "            for result in results:\n",
    "                boxes = result.boxes\n",
    "                if boxes is not None and len(boxes) > 0:\n",
    "                    for box in boxes:\n",
    "                        # ì‹ ë¢°ë„ 0.8 ì´ìƒë§Œ ì²˜ë¦¬ (ë¹„ë””ì˜¤ ì½”ë“œì™€ ë™ì¼)\n",
    "                        if box.cls[0] == 0 and box.conf[0] > 0.8:\n",
    "                            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                            confidence = float(box.conf[0])\n",
    "                            \n",
    "                            # ì–¼êµ´ ì˜ì—­ ì¶”ì • (ìƒë°˜ì‹ ì˜ ìƒìœ„ 1/3 ì •ë„)\n",
    "                            face_height = int((y2 - y1) * 0.3)\n",
    "                            face_y1 = y1\n",
    "                            face_y2 = y1 + face_height\n",
    "                            face_x1 = x1 + int((x2 - x1) * 0.2)\n",
    "                            face_x2 = x2 - int((x2 - x1) * 0.2)\n",
    "                            \n",
    "                            # ìœ íš¨í•œ ì˜ì—­ì¸ì§€ í™•ì¸\n",
    "                            if (face_y2 > face_y1 and face_x2 > face_x1 and \n",
    "                                face_y1 >= 0 and face_x1 >= 0 and \n",
    "                                face_y2 < frame.shape[0] and face_x2 < frame.shape[1]):\n",
    "                                \n",
    "                                face_crop = frame_rgb[face_y1:face_y2, face_x1:face_x2]\n",
    "                                \n",
    "                                if face_crop.size > 0:\n",
    "                                    # ê°ì • ì˜ˆì¸¡\n",
    "                                    emotion_results = predict_emotion(face_crop)\n",
    "                                    face_detected = True\n",
    "                                    \n",
    "                                    # ìƒìœ„ 2ê°œ ê°ì •ì— ê°€ì¤‘ì¹˜ ì ìš© (ë¹„ë””ì˜¤ ì½”ë“œì™€ ë™ì¼)\n",
    "                                    if len(emotion_results) >= 2:\n",
    "                                        first_emotion = emotion_results[0]['label']\n",
    "                                        second_emotion = emotion_results[1]['label']\n",
    "                                        first_score = emotion_results[0]['score']\n",
    "                                        second_score = emotion_results[1]['score']\n",
    "                                        \n",
    "                                        # ì‹ ë¢°ë„ ì°¨ì´ê°€ í´ìˆ˜ë¡ ì²« ë²ˆì§¸ì— ë” í° ê°€ì¤‘ì¹˜\n",
    "                                        score_ratio = first_score / (second_score + 0.01)\n",
    "                                        if score_ratio > 2.0:  # 2ë°° ì´ìƒ ì°¨ì´ë‚˜ë©´\n",
    "                                            current_analysis_emotions.extend([first_emotion] * 3)  # 3ë°° ê°€ì¤‘ì¹˜\n",
    "                                            current_analysis_emotions.extend([second_emotion])\n",
    "                                        else:\n",
    "                                            current_analysis_emotions.extend([first_emotion])\n",
    "                                            current_analysis_emotions.extend([second_emotion])\n",
    "                                    \n",
    "                                    analysis_results.append(f\"ğŸ‘¤ ì‚¬ëŒ íƒì§€ë¨ (ì‹ ë¢°ë„: {confidence:.2f})\")\n",
    "                                    analysis_results.append(f\"ğŸ“Š ê°ì • ë¶„ì„ ê²°ê³¼:\")\n",
    "                                    \n",
    "                                    for i, emotion in enumerate(emotion_results[:3]):\n",
    "                                        emoji_map = {\n",
    "                                            'Angry': 'ğŸ˜¡', 'Disgust': 'ğŸ¤¢', 'Fear': 'ğŸ˜¨',\n",
    "                                            'Happy': 'ğŸ˜„', 'Neutral': 'ğŸ˜', 'Sad': 'ğŸ˜­', 'Surprise': 'ğŸ˜®'\n",
    "                                        }\n",
    "                                        emoji = emoji_map.get(emotion['label'], 'â“')\n",
    "                                        analysis_results.append(\n",
    "                                            f\"  {i+1}. {emoji} {emotion['label']}: {emotion['score']:.1%}\"\n",
    "                                        )\n",
    "                                    break\n",
    "            \n",
    "            # ê°ì • ë°ì´í„° ì €ì¥ (ì–¼êµ´ì´ ê°ì§€ëœ ê²½ìš°ë§Œ)\n",
    "            if face_detected and current_analysis_emotions:\n",
    "                # ì „ì²´ ê°ì • ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n",
    "                all_emotions.extend(current_analysis_emotions)\n",
    "                \n",
    "                # ê°ì •ë³„ ì¹´ìš´íŠ¸ ì—…ë°ì´íŠ¸\n",
    "                for emotion in current_analysis_emotions:\n",
    "                    if emotion in emotion_statistics:\n",
    "                        emotion_statistics[emotion] += 1\n",
    "                \n",
    "                # ë¶„ì„ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€\n",
    "                analysis_history.append({\n",
    "                    'timestamp': datetime.datetime.now().isoformat(),\n",
    "                    'emotions': current_analysis_emotions.copy(),\n",
    "                    'confidence': confidence\n",
    "                })\n",
    "                \n",
    "                # ê°€ì¤‘ì¹˜ ì ìš©ëœ ê°ì • ì •ë³´ ì¶”ê°€\n",
    "                analysis_results.append(f\"\\nğŸ”¢ ê°€ì¤‘ì¹˜ ì ìš©ëœ ê°ì •: {', '.join(current_analysis_emotions)}\")\n",
    "            \n",
    "            if not face_detected:\n",
    "                analysis_results.append(\"ğŸ‘¤ ì‚¬ëŒì´ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "                analysis_results.append(\"ğŸ’¡ ì¹´ë©”ë¼ì— ì–¼êµ´ì´ ì˜ ë³´ì´ë„ë¡ ì¡°ì •í•´ì£¼ì„¸ìš”.\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            analysis_results.append(f\"âŒ ì–¼êµ´ íƒì§€ ì˜¤ë¥˜: {str(e)}\")\n",
    "        \n",
    "        # ë¶„ì„ ì‹œê°„ ì¶”ê°€\n",
    "        analysis_results.append(f\"\\nâ° ë¶„ì„ ì‹œê°„: {datetime.datetime.now().strftime('%H:%M:%S')}\")\n",
    "        \n",
    "        # ëˆ„ì  í†µê³„ ì •ë³´ ì¶”ê°€\n",
    "        if all_emotions:\n",
    "            analysis_results.append(f\"\\nğŸ“Š ëˆ„ì  ë¶„ì„ í†µê³„:\")\n",
    "            for emotion, count in emotion_statistics.items():\n",
    "                if count > 0:\n",
    "                    emoji_map = {\n",
    "                        'Angry': 'ğŸ˜¡', 'Disgust': 'ğŸ¤¢', 'Fear': 'ğŸ˜¨',\n",
    "                        'Happy': 'ğŸ˜„', 'Neutral': 'ğŸ˜', 'Sad': 'ğŸ˜­', 'Surprise': 'ğŸ˜®'\n",
    "                    }\n",
    "                    emoji = emoji_map.get(emotion, 'â“')\n",
    "                    analysis_results.append(f\"  {emoji} {emotion}: {count}íšŒ\")\n",
    "        \n",
    "        return \"\\n\".join(analysis_results)\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"âŒ ì´ë¯¸ì§€ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\"\n",
    "\n",
    "print(\"âœ… AI ë¶„ì„ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88373194",
   "metadata": {},
   "source": [
    "# 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847c2c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… FastAPI ë¼ìš°íŠ¸ê°€ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "ğŸš€ AI ê°ì • ë¶„ì„ ì„œë²„ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤!\n",
      "ğŸ“± ì›¹í˜ì´ì§€ ì ‘ì†: http://127.0.0.1:8000\n",
      "ğŸ“Š ëª¨ë¸ ìƒíƒœ í™•ì¸: http://127.0.0.1:8000/model-status\n",
      "ğŸ“š API ë¬¸ì„œ: http://127.0.0.1:8000/docs\n",
      "\n",
      "ğŸ¤– ì§€ì› ê¸°ëŠ¥:\n",
      "- ì‹¤ì‹œê°„ ì›¹ìº  ìº¡ì²˜\n",
      "- YOLO ê¸°ë°˜ ì–¼êµ´/ì‚¬ëŒ íƒì§€ (í•„ìˆ˜)\n",
      "- ViT ê¸°ë°˜ 7ê°€ì§€ ê°ì • ë¶„ì„ (1:1 íŒ¨ë”© + 224x224)\n",
      "- ì‹¤ì‹œê°„ ê²°ê³¼ í‘œì‹œ\n",
      "\n",
      "âš ï¸  ì£¼ì˜: ì²« ì‹¤í–‰ ì‹œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œë¡œ ì‹œê°„ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [29144]\n",
      "INFO:     Waiting for application startup.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!\n",
      "ì‚¬ìš© ì¥ì¹˜: cpu\n",
      "INFO:     127.0.0.1:7610 - \"GET / HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:  Unsupported upgrade request.\n",
      "WARNING:  No supported WebSocket library detected. Please use \"pip install 'uvicorn[standard]'\", or install 'websockets' or 'wsproto' manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:4684 - \"GET /ws HTTP/1.1\" 404 Not Found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:  Unsupported upgrade request.\n",
      "WARNING:  No supported WebSocket library detected. Please use \"pip install 'uvicorn[standard]'\", or install 'websockets' or 'wsproto' manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:3629 - \"GET /ws HTTP/1.1\" 404 Not Found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:  Unsupported upgrade request.\n",
      "WARNING:  No supported WebSocket library detected. Please use \"pip install 'uvicorn[standard]'\", or install 'websockets' or 'wsproto' manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:9305 - \"GET /ws HTTP/1.1\" 404 Not Found\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: FastAPI ë¼ìš°íŠ¸ ì •ì˜ \n",
    "@app.get(\"/\", response_class=HTMLResponse)\n",
    "async def main_page(request: Request):\n",
    "    return templates.TemplateResponse(\"index.html\", {\"request\": request})\n",
    "\n",
    "@app.post(\"/analyze\")\n",
    "async def analyze_image(file: UploadFile = File(...)):\n",
    "    try:\n",
    "        if not file.content_type.startswith(\"image/\"):\n",
    "            raise HTTPException(status_code=400, detail=\"ì´ë¯¸ì§€ íŒŒì¼ë§Œ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤.\")\n",
    "        \n",
    "        image_data = await file.read()\n",
    "        print(f\"[DEBUG] ì´ë¯¸ì§€ ë°”ì´íŠ¸ í¬ê¸°: {len(image_data)}\")\n",
    "        analysis_result = analyze_webcam_image(image_data)\n",
    "        print(f\"[DEBUG] ë¶„ì„ ê²°ê³¼:\\n{analysis_result}\")\n",
    "        \n",
    "        return JSONResponse(content={\n",
    "            \"status\": \"success\",\n",
    "            \"analysis\": analysis_result,\n",
    "            \"image_size\": len(image_data)\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "        return JSONResponse(content={\n",
    "            \"status\": \"error\",\n",
    "            \"analysis\": f\"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}\"\n",
    "        }, status_code=500)\n",
    "\n",
    "@app.get(\"/model-status\")\n",
    "async def model_status():\n",
    "    global emotion_model, yolo_model\n",
    "    status = {\n",
    "        \"emotion_model\": emotion_model is not None,\n",
    "        \"yolo_model\": yolo_model is not None,\n",
    "        \"device\": device\n",
    "    }\n",
    "    return JSONResponse(content=status)\n",
    "\n",
    "@app.get(\"/emotion-statistics\")\n",
    "async def get_emotion_statistics():\n",
    "    \"\"\"ê°ì • ë¶„ì„ í†µê³„ ì¡°íšŒ\"\"\"\n",
    "    global all_emotions, emotion_statistics, analysis_history\n",
    "    \n",
    "    return JSONResponse(content={\n",
    "        \"total_analyses\": len(analysis_history),\n",
    "        \"total_emotions\": len(all_emotions),\n",
    "        \"emotion_counts\": emotion_statistics,\n",
    "        \"recent_analyses\": analysis_history[-10:] if len(analysis_history) > 10 else analysis_history\n",
    "    })\n",
    "\n",
    "@app.post(\"/reset-statistics\")\n",
    "async def reset_statistics():\n",
    "    \"\"\"ê°ì • ë¶„ì„ í†µê³„ ì´ˆê¸°í™”\"\"\"\n",
    "    global all_emotions, emotion_statistics, analysis_history\n",
    "    \n",
    "    all_emotions.clear()\n",
    "    analysis_history.clear()\n",
    "    for emotion in emotion_statistics:\n",
    "        emotion_statistics[emotion] = 0\n",
    "    \n",
    "    return JSONResponse(content={\"status\": \"success\", \"message\": \"í†µê³„ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.\"})\n",
    "\n",
    "@app.get(\"/export-emotions\")\n",
    "async def export_emotions():\n",
    "    \"\"\"ê°ì • ë°ì´í„° ë‚´ë³´ë‚´ê¸° (ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì „ í™•ì¸ìš©)\"\"\"\n",
    "    global all_emotions, emotion_statistics, analysis_history\n",
    "    \n",
    "    return JSONResponse(content={\n",
    "        \"all_emotions\": all_emotions,\n",
    "        \"statistics\": emotion_statistics,\n",
    "        \"history\": analysis_history,\n",
    "        \"export_time\": datetime.datetime.now().isoformat()\n",
    "    })\n",
    "\n",
    "@app.websocket(\"/ws\")\n",
    "async def websocket_endpoint(websocket: WebSocket):\n",
    "    await websocket.accept()\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            # JSON í˜•íƒœë¡œ ë°›ê¸°\n",
    "            data = await websocket.receive_json()\n",
    "            \n",
    "            # base64 ì´ë¯¸ì§€ ë””ì½”ë”©\n",
    "            if 'image' in data:\n",
    "                image_b64 = data['image']\n",
    "                image_bytes = base64.b64decode(image_b64)\n",
    "                \n",
    "                # ê¸°ì¡´ ë¶„ì„ í•¨ìˆ˜ ì‚¬ìš©\n",
    "                result = analyze_webcam_image(image_bytes)\n",
    "                \n",
    "                # ê²°ê³¼ ì „ì†¡\n",
    "                await websocket.send_json({\"analysis\": result})\n",
    "            \n",
    "    except WebSocketDisconnect:\n",
    "        pass\n",
    "    except Exception as e:\n",
    "        print(f\"WebSocket ì˜¤ë¥˜: {e}\")\n",
    "        await websocket.send_json({\"analysis\": f\"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}\"})\n",
    "\n",
    "print(\"âœ… FastAPI ë¼ìš°íŠ¸ê°€ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# ì„œë²„ ì‹¤í–‰ í•¨ìˆ˜\n",
    "def run_server():\n",
    "    uvicorn.run(app, host=\"127.0.0.1\", port=8000, log_level=\"info\")\n",
    "\n",
    "# ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì„œë²„ ì‹¤í–‰\n",
    "server_thread = threading.Thread(target=run_server, daemon=True)\n",
    "server_thread.start()\n",
    "\n",
    "print(\"ğŸš€ AI ê°ì • ë¶„ì„ ì„œë²„ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "print(\"ğŸ“± ì›¹í˜ì´ì§€ ì ‘ì†: http://127.0.0.1:8000\")\n",
    "print(\"ğŸ“Š ëª¨ë¸ ìƒíƒœ í™•ì¸: http://127.0.0.1:8000/model-status\")\n",
    "print(\"ğŸ“š API ë¬¸ì„œ: http://127.0.0.1:8000/docs\")\n",
    "print(\"\\nğŸ¤– ì§€ì› ê¸°ëŠ¥:\")\n",
    "print(\"- ì‹¤ì‹œê°„ ì›¹ìº  ìº¡ì²˜\")\n",
    "print(\"- YOLO ê¸°ë°˜ ì–¼êµ´/ì‚¬ëŒ íƒì§€ (í•„ìˆ˜)\")\n",
    "print(\"- ViT ê¸°ë°˜ 7ê°€ì§€ ê°ì • ë¶„ì„ (1:1 íŒ¨ë”© + 224x224)\")\n",
    "print(\"- ì‹¤ì‹œê°„ ê²°ê³¼ í‘œì‹œ\")\n",
    "print(\"\\nâš ï¸  ì£¼ì˜: ì²« ì‹¤í–‰ ì‹œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œë¡œ ì‹œê°„ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc13fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import signal\n",
    "\n",
    "# í˜„ì¬ í”„ë¡œì„¸ìŠ¤ì˜ PID ì–»ê¸°\n",
    "current_pid = os.getpid()\n",
    "print(f\"í˜„ì¬ í”„ë¡œì„¸ìŠ¤ PID: {current_pid}\")\n",
    "\n",
    "# ìì‹ ì„ ì¢…ë£Œ\n",
    "os.kill(current_pid, signal.SIGKILL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5681c7bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opencv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
